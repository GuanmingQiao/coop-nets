%
% File acl2012.tex
%
% Contact: Maggie Li (cswjli@comp.polyu.edu.hk), Michael White (mwhite@ling.osu.edu)
%%
%% Based on the style files for ACL2008 by Joakim Nivre and Noah Smith
%% and that of ACL2010 by Jing-Shin Chang and Philipp Koehn


\documentclass[11pt,letterpaper]{article}
\usepackage[letterpaper]{geometry}
\usepackage{acl2012}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{todonotes}
\usepackage{url}
\usepackage{color}
\usepackage[inline]{enumitem}
\usepackage{booktabs}
\usepackage[font=small]{caption}
\usepackage[font=small]{subcaption}
\usepackage{colortbl}
\usepackage{tikz}
\usepackage{comment}
\usepackage{epstopdf}
\usetikzlibrary{shapes.geometric}
\makeatletter
\newcommand{\@BIBLABEL}{\@emptybiblabel}
\newcommand{\@emptybiblabel}[1]{}
\makeatother
\usepackage[hidelinks,draft]{hyperref}
\DeclareMathOperator*{\argmax}{arg\,max}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Code to use with NAACL/ACL style files to simulate natbib's
% \citealt, which prints citations with no parentheses. This should
% work if pasted into the preamble. \cite, \newcite, and \shortcite
% should continue to work as before.

\makeatletter

\def\citealt{\def\citename##1{{\frenchspacing##1} }\@internalcitec}

\def\@citexc[#1]#2{\if@filesw\immediate\write\@auxout{\string\citation{#2}}\fi
  \def\@citea{}\@citealt{\@for\@citeb:=#2\do
    {\@citea\def\@citea{;\penalty\@m\ }\@ifundefined
       {b@\@citeb}{{\bf ?}\@warning
       {Citation `\@citeb' on page \thepage \space undefined}}%
{\csname b@\@citeb\endcsname}}}{#1}}

\def\@internalcitec{\@ifnextchar [{\@tempswatrue\@citexc}{\@tempswafalse\@citexc[]}}

\def\@citealt#1#2{{#1\if@tempswa, #2\fi}}

\makeatother

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newcommand{\term}{\textit}

\newcommand{\word}{\textit}

\newcommand{\eq}[1]{(\ref{#1})}

\newcommand{\Listener}{L}
\newcommand{\Speaker}{S}
\newcommand{\utt}{u}
\newcommand{\uttlen}{N}
\newcommand{\referent}{c}
\newcommand{\context}{C}
\newcommand{\contextlen}{K}
\newcommand{\target}{t}
\newcommand{\numsamples}{m}
\newcommand{\feat}{f}
\renewcommand{\|}{\mid}
\newcommand{\best}[1]{\textbf{#1}}
\newcommand{\oracle}[1]{\textit{#1}}

\newcommand{\set}[1]{\left\{#1\right\}}

\newcommand{\Secref}[1]{Section~\ref{#1}}
\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\dashsecref}[2]{Sections~\ref{#1}--\ref{#2}}
\newcommand{\Figref}[1]{Figure~\ref{#1}}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\dashfigref}[2]{Figures~\ref{#1}--\ref{#2}}
\newcommand{\Tabref}[1]{Table~\ref{#1}}
\newcommand{\tabref}[1]{Table~\ref{#1}}
\newcommand{\pararef}[1]{\textbf{#1}}

\definecolor{ourlightblue}{HTML}{03A9F4}
\definecolor{ourgreen}{HTML}{4D8951}
\definecolor{oursteelblue}{HTML}{9BB8D7}
\definecolor{ourorange}{HTML}{FDBA58}

\newcommand{\ourlightblue}[1]{{\color{ourlightblue}#1}}
\newcommand{\ourgreen}[1]{{\color{ourgreen}#1}}
\newcommand{\oursteelblue}[1]{{\color{oursteelblue}#1}}
\newcommand{\ourorange}[1]{{\color{ourorange}#1}}

\newcolumntype{P}[1]{>{\raggedright\arraybackslash}p{#1}}

\newcommand{\colorPatch}[2][xxxx]{
  \colorbox[HTML]{#2}{{\color[HTML]{#2}#1}}}

\newcommand{\colorSolo}[2]{
  \negthickspace\colorPatch{#1} & #2}

\newcommand{\colorContext}[4]{
  \framebox{\negthickspace\colorPatch{#1}} & \colorPatch{#2} & \colorPatch{#3} & #4}


\newcommand{\colorContextGame}[3]{
  \negthickspace\colorPatch{#1} & \colorPatch{#2} & \colorPatch{#3}}

\newcommand{\colorContextNarrow}[3]{
  \colorPatch[xx]{#1}, \colorPatch[xx]{#2}, \colorPatch[xx]{#3}}

\newcommand{\todocheck}[1]{\textcolor{red}{#1}}
% For filling out boxed table cells to equal length
\newcommand{\gzz}{\phantom{$<$00}}
\newcommand{\gz}{\phantom{$<$0}}
\newcommand{\zz}{\phantom{00}}
\newcommand{\z}{\phantom{0}}
\newcommand{\p}{\phantom{\%}}

\newcommand{\cond}{\emph}

\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{Colors in Context: A Pragmatic Neural Model for \\
Grounded Language Understanding}

\author{}
% First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   {\tt email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}


We present a two-layer pragmatic model of referring expression interpretation:
a listener modeling a speaker modeling a listener, where the base listener is a
recurrent neural network classifier. Our model outperforms the
base listener at a task of color reference understanding, evaluated on a
newly-collected corpus of human utterances in color reference games.
Our experiments show that the primary benefits from pragmatic reasoning come
in the hardest cases: when the model must distinguish colors that are very
similar, or when there are few utterances available that adequately express
the target color.

\end{abstract}

\section{Introduction} \label{sec:intro}

% Grounded language use requires adapting language to the context at hand: trying
% to identify an object by describing it as ``blue'' will be unhelpful if all
% accessible objects are some shade of blue. People assume their interlocutors know
% this and adapt their interpretation of grounded language accordingly. This intuition
% is captured in game-theoretic and probabilistic models of pragmatic
% language understanding \cite{Jaeger2012,Frank2012}.

Human communication is \emph{situated}. In using language, we are sensitive
to context and our interlocutors' expectations, when choosing our
utterances (as speakers) and when interpreting the utterances we hear (as listeners).
Visual referring tasks exercise this complex process of grounding, 
in the environment and in
our mental models of each other, and thus provide a valuable test-bed for
computational models of production and comprehension.

\Tabref{table:examples} illustrates the situated nature of
reference understanding with descriptions of colors from a task-oriented
dialogue corpus we
introduce with this paper. In these dialogues, the speaker is trying
to identify their (privately assigned) target color for the
listener. In context~1, the comparative \word{darker} implicitly
refers to both the target (boxed) and one of the other colors. In
contexts 2 and 3, the target color is the same, but the distractors
led the speaker to choose different basic color terms. In
context~4, \word{blue} is a pragmatic choice even though two colors are
shades of blue, because the interlocutors assume about each other that
they find the target color a more prototypical representative of blue
and would prefer other descriptions (\word{teal}, \word{cyan}) for the middle color.
The fact that \word{blue} is the utterance of choice in three of these
four cases re-enforces the context dependence of color descriptions.

\begin{table}
  \centering
  \setlength{\tabcolsep}{4pt}
  \begin{tabular}[c]{r@{. \ } ccc l}
    \toprule
    \multicolumn{4}{c}{Context} & Utterance \\
    \midrule
    1&\colorContext{2421DE}{605DA2}{0144FE}{darker blue}\\
    2&\colorContext{5866A7}{2DD2BC}{C23D5A}{Purple}\\
    3&\colorContext{5866A7}{9953AC}{2DD2A6}{blue}\\
    4&\colorContext{3884C7}{02F9FD}{9E6461}{blue}\\
    \bottomrule
  \end{tabular}
  \caption{Examples of color reference in context. The target color
    is boxed. The speaker's description is shaped not only
    by this target, but also by the other context colors and their
    relationships.}
  \label{table:examples}
\end{table}

In this paper, we present a scalable, learned model of pragmatic
language understanding. At its core, the model is a version of the
Rational Speech Acts (RSA) model \cite{Frank2012}, in which agents
reason recursively about each other's expectations and intentions to
communicate more effectively than literal semantic agents could. In
most work on RSA, the literal semantic agents use fixed message sets
and stipulated grammars, which is a barrier to experiments in
linguistically complex domains. In our formulation, the literal
semantic agents are recurrent neural networks (RNNs) that produce and
interpret color descriptions in context. These models are learned from
data and scale easily to large data sets containing novel utterances.
The RSA recursion is then defined in terms of these base agents: the
\emph{pragmatic speaker} produces utterances based on a literal RNN
listener \cite{AndreasKlein16_NeuralPragmatics}, and the
\emph{pragmatic listener} interprets utterances based on the pragmatic
speaker's behavior.

% We evaluate our model in a \term{reference game} task: given a context consisting
% of a set of potential \term{referents}, one referent is selected as the \term{target}
% and its identity indicated secretly to the \term{speaker}. The speaker then must
% convey the identity of the target to a \term{listener}. Our model plays the
% role of the listener in this game, attempting to resolve a human utterance
% to the correct referent. The reference game setting tests the ability of our model
% to do two distinct forms of pragmatic reasoning: the model must consider
% not only world context in the form of the set of available referents,
% but also linguistic context in the form of the speaker's goals and constraints in
% describing the target.

We evaluate this model with a new, psycholinguistically motivated corpus of reference games in which
the referents are patches of color. We focus on the effectiveness of
our listener agents for simplicity. (In our model, listeners depend on
speakers, so this implicitly evaluates our speakers as well.) The
corpus includes 942 complete games with 47,100 utterances produced by
humans playing these games with real-time feedback. The linguistic
behavior of the players exhibits many of the intricacies of language
in general, including not just the context dependence and cognitive
complexity discussed above, but also compositionality, vagueness, and
ambiguity. While many previous data sets feature descriptions of
individual colors \cite{Cook2005,Munroe2010,Kawakami2016}, situating
colors in context elicits much greater variety in language use,
including negations, comparatives and superlatives, and increased use
of metaphor and shared associations.

Experiments on this data show that our pragmatic model improves accuracy
in interpreting human-produced descriptions over the RNN base agent
alone. Moreover, the improvements from pragmatic reasoning come primarily
in the hardest cases. By this we mean
%
\begin{enumerate*}[label=(\arabic*)]
\item contexts featuring colors that are very similar, thus requiring
  the interpretation of descriptions that convey fine distinctions;
  and
\item target colors that most referring expressions fail to identify,
  whether due to a lack of adequate descriptive terms or a consistent
  bias against the color in the RNN listener.
\end{enumerate*}


\section{Task and data collection}\label{sec:corpus}

\todocheck{It would be good to open this section with a sentence or
  two on the background of these tangrams games, articulating why
  they have been important in cognitive science. That will help to
  begin developing the theme that we are doing cognitive science as
  well as NLP.}
We compare agents on a task of language understanding in a reference
game \cite{Rosenberg:Cohen:1964,Clark:Wilkes-Gibbs:1986,Paetzel-etal:2014}.
A listener agent $\Listener$ is presented with a context set of colors
$\context = \{\referent_{i}, i=1..\contextlen\}$ and an English
utterance $\utt$ that describes the target color
$\referent_{\target} \in \context$. The agent does not know the index
$\target$ of the target color and must guess it from $\utt$.

To obtain a corpus of natural color reference data across varying
contexts, we recruited \todocheck{\#} unique participants from Amazon
Mechanical Turk to play 1,117 real-time, multi-player communication
games of 50 rounds each, using the open-source framework of
\newcite{Hawkins15_RealTimeWebExperiments}.  \todocheck{After non-native
  English speakers and incomplete games were excluded, we were left
  with a corpus of 47,100 utterances across 942 games [report modal
  number of games per turker?]}. Participants were sorted into dyads,
assigned the role of speaker or listener, and placed in a game
environment containing a chat box and an array of three color patches
(\figref{fig:taskScreenshot}).

\begin{figure}
\includegraphics[scale = .2]{figures/speakerView.png}
\caption{Example trial in corpus collection task, from speaker's
  perspective. \todocheck{The rationale for the black background could
    be given (just) in the caption.}}
\label{fig:taskScreenshot}
\end{figure}

On each round, one of the three colors was chosen to be the target and
highlighted for the speaker. They were instructed to communicate this
information to the listener, who could then click on one of the colors
to advance to the next trial. Both participants were free to use the
chat box at any point.

\newcommand{\Lex}{\mathcal{L}}
\newcommand{\Costs}{\kappa}
\newcommand{\Messages}{U}
\newcommand{\targetPrior}{P}

\begin{figure*}[t]
  \centering
  %
  \begin{subfigure}[t]{0.3\textwidth}
    \centering
    \begin{tabular}{lr@{\hskip 5pt}r@{\hskip 5pt}r@{}r}
    \toprule
     & \colorContextGame{3884C7}{02F9FD}{9E6461}{} \\
    \midrule
    blue & \best{50}\% & \best{50}\p & 0\p \\
    teal & 0\p & \best{100}\p & 0\p \\
    dull & \best{50}\p & 0\p & \best{50}\p \\
    \bottomrule
    \end{tabular}
    \caption{The literal listener $l_{0}$ chooses colors that match the
             literal semantics of the utterance; other than that, it
             guesses randomly.}
    \label{fig:basic-rsa:l0}
  \end{subfigure}
  %
  \hfill
  %
  \begin{subfigure}[t]{0.3\textwidth}
    \centering
    \begin{tabular}{lr@{\hskip 5pt}r@{\hskip 5pt}r@{}r}
    \toprule
     & \colorContextGame{3884C7}{02F9FD}{9E6461}{} \\
    \midrule
    blue & \best{50}\p & 33\p & 0\p \\
    teal & 0\p & \best{67}\p & 0\p \\
    dull & \best{50}\p & 0\p & \best{100}\p \\
    \bottomrule
    \end{tabular}
    \caption{The pragmatic speaker $s_{1}$ soft-maximizes the informativity of
             its utterances. (For simplicity, $\alpha = 1$ and
             $\Costs(\utt) = 0$.)}
    \label{fig:basic-rsa:s1}
  \end{subfigure}
  %
  \hfill
  %
  \begin{subfigure}[t]{0.3\textwidth}
    \centering
    \begin{tabular}{lr@{\hskip 5pt}r@{\hskip 5pt}r@{}r}
    \toprule
     & \colorContextGame{3884C7}{02F9FD}{9E6461}{} \\
    \midrule
    blue & \best{60}\p & 40\p & 0\p \\
    teal & 0\p & \best{100}\p & 0\p \\
    dull & 33\p & 0\p & \best{67}\p \\
    \bottomrule
    \end{tabular}
    \caption{The pragmatic listener $l_{1}$ uses Bayes' rule to infer the target
             using the speaker's utterance as evidence.}
    \label{fig:basic-rsa:l1}
  \end{subfigure}
  %
  \caption{An example of RSA applied to a color reference task (literal semantics and alternative utterances simplified for demonstration).}
  \label{fig:basic-rsa}
\end{figure*}

To ensure a range of difficulty, we randomly interspersed an equal
number of trials from three different conditions:
%
\begin{enumerate*}[label=(\arabic*)]%
\item \cond{close}, where colors were all within a distance of
  $\theta$ from one another but still perceptible,\footnote{We used the
    most recent CIEDE standard to measure color differences, which is
    calibrated to human vision \cite{SharmaWuDalal05_DeltaE}. All
    distances were constrained to be larger than a lower bound of
    $\epsilon = 5$ to ensure perceptible differences, and we used a
    threshold value of $\theta = 20$ to create conditions.} 
\item \cond{split}, where one distractor was within a distance of
  $\theta$ of the target, but the other distractor was further than
  $\theta$, and
\item \cond{far}, where all colors were farther than $\theta$ from one
  another. Colors were rejection sampled uniformly from RGB (red,
  green, blue) space to meet these constraints.
\end{enumerate*}
\todocheck{Let's include here the number of each type of trial in the
final corpus. Presumably, it's about 1/3 each, but it would be good
to be explicit.}


\section{Models}

Our model is intended as a scalable version of RSA. We first define the
basic RSA model as applied to the
color reference games introduced in \secref{sec:corpus}; a worked example
is shown in \figref{fig:basic-rsa}. The starting point of RSA is a model
of a \term{literal listener}:
\begin{align}
  l_{0}(\target \| \utt, \Lex)
  &\propto
  \Lex(\utt, \target) \targetPrior(\target)
\end{align}
where $\target$ is a color in the context
set $\context$, $\utt$ is a message drawn from a set of possible
utterances $\Messages$, $\targetPrior$ is a prior over colors,
and $\Lex(\utt, \target)$ is a semantic interpretation function that
takes the value $1$ if $\utt$ is true of $\target$, else $0$.
\Figref{fig:basic-rsa:l0} summarizes a literal listener defined for 
a very simple context in which $\Messages=\set{\word{blue},
  \word{teal}, \word{dull}}$, $\context =
\set{\colorContextNarrow{3884C7}{02F9FD}{9E6461}{}}$, 
and the prior $\targetPrior$ over these context elements is flat.


To derive a model of a \term{pragmatic speaker} (\figref{fig:basic-rsa:s1}), 
RSA postulates that
a speaker behaves according to a distribution that soft-maximizes
a utility function that rewards informativity and penalizes cost:
\begin{align}
  s_{1}(\utt \| \target, \Lex)
  &\propto
  e^{\alpha\log(l_{0}(\target \| \utt, \Lex)) - \Costs(\utt)}
  \label{eq:rsa-s1}
\end{align}
Here, $\Costs$ is a real-valued cost function on utterances, and
$\alpha \in [0,\infty)$ is an inverse temperature parameter governing
the ``rationality'' of the speaker model.
A large $\alpha$ means that the pragmatic speaker is expected to choose
the most informative utterance (minus cost)
consistently, whereas a small $\alpha$ means the
speaker is modeled as choosing suboptimal utterances frequently.

Finally, a \term{pragmatic listener} (\figref{fig:basic-rsa:l1})
interprets utterances by reasoning about the behavior of the pragmatic speaker:
\begin{align}
  l_{1}(\target \| \utt, \Lex)
  &\propto
  s_{1}(\utt \| \target, \Lex) \targetPrior(\target)
  \label{eq:rsa-l1}
\end{align}

The $\alpha$ parameter of the speaker indirectly affects the
listener's interpretations: the more reliable the speaker is in
choosing the optimal utterance for a referent, the more the listener
will take deviations from an optimal utterance as a signal to choose a
different referent.

The most important feature of this model is that the pragmatic
listener $l_{1}$ reasons, not about the semantic interpretation
function $\Lex$ directly, but rather about a speaker who reasons about
a listener who reasons about $\Lex$ directly. The back-and-forth
nature of this interpretive process mirrors the
cooperative principle of conversational implicature \cite{Grice75} and
reflects more general ideas from Bayesian cognitive modeling
\cite{Tenenbaum-etal:2011}. The model and its variants have been shown
to capture a wide range of pragmatic phenomena in a cognitively
realistic manner
\cite{Goodman2013,Smith:Goodman:Frank:2013,Kao-etal:2014,Bergen:Levy:Goodman:2014},
and the central Bayesian calculation has proven useful in a variety of
communicative domains
\cite{Tellex2014a,Vogel:Potts:Jurafsky:2013}.

The scalability problems of RSA stem from the set of messages
$\Messages$ and the interpretation function $\Lex$. In most versions of
RSA, these are specified by hand (but see \citealt{Monroe2015}).  This
presents a serious practical obstacle to applying RSA to large data
sets containing realistic utterances. Relatedly, the truth-functional
nature of $\Lex$ itself forces us to confront challenging issues of
vagueness and semantic uncertainty at every turn. The set $\Messages$
also raises a more fundamental issue: if this set is not finite (as
one would expect from a compositional semantic system), then in general there is
no exact way to normalize the $s_{1}$ scores, since the denominator
must sum over all messages.

Over the next few subsections, we overcome these obstacles by
replacing $l_{0}$ with an RNN-based listener agent, and by defining an
RNN-based speaker agent (a kind of $s_{0}$ agent) to provide sample
utterances that let us approximate the normalization required for
defining an $s_{1}$ agent.

\begin{figure*}[t]
  \centering
  %
   \begin{subfigure}[b]{0.48\textwidth}
    \centering
    \input{input-neural-listener}
    \caption{The $\Listener_{0}$ agent processes a color description
      sequentially. The final representation is transformed into a
      Gaussian distribution in color space, which is used to score the
      context colors.}
    \label{fig:model:listener}
  \end{subfigure}
  %
  \hfill
  %
  \begin{subfigure}[b]{0.48\textwidth}
    \centering
    \input{input-neural-speaker}
    \caption{The $\Speaker_{0}$ agent processes the target color
      $c_{T}$ in context and produces a color description
      sequentially. Each step in production is conditioned by the
      final contextual representation $h$ and the previous word
      produced.}
    \label{fig:model:speaker}
  \end{subfigure}
  %
  \caption{The neural base speaker and listener agents.}
  \label{fig:model}
\end{figure*}

\subsection{Representing colors}

Each color is represented in its simplest form as a three-dimensional vector in
RGB space; our models transform this to HSV (hue, saturation,
value) as a preprocessing step. These HSV vectors are then Fourier-transformed as in \newcite{MonroeGoodmanPotts16_Color} before being fed as input to the RNNs;
a color
$(h, s, v)$ is mapped to a higher-dimensional\footnote{For the listener, we restrict $j,k$ to $\{0, 1\}$ and $\ell$ to 0, as we
found this improved performance slightly.} vector $\feat$:
\begin{align*}
\hat{\feat}_{jk\ell} &= \exp \left[-2\pi i \left(jh^* + ks^* + \ell v^*\right)\right] \\
\feat &= \begin{bmatrix}
  \Re{\hat{\feat}} & \Im{\hat{\feat}}
\end{bmatrix}\qquad j,k,\ell \in \{0,1,2\}
\end{align*}
where $(h^*, s^*, v^*) = (h / 360, s / 200, v/200)$.
The Fourier transformation is meant to help the models identify non-convex components
of denotations of color language, particularly periodic components.

\subsection{Base listener}

Our base listener agent $\Listener_0$ is an LSTM encoder model that produces a Gaussian
distribution over colors in a transformed representation space (\figref{fig:model:listener}).
The words in the input are embedded in a 100-dimensional vector space. Word embeddings
are initialized to random normally-distributed vectors ($\mu = 0$, $\sigma = 0.01$)
and trained. The sequence of word vectors are
used as input to an LSTM with 100-dimensional hidden state, and a linear
transformation is applied to the output representation to produce a covariance matrix
and mean vector for a Gaussian distribution in color representation space.
The Gaussian scores for each of the $\contextlen$ context colors are normalized to
produce a probability distribution over the context colors. We denote this probability
distribution by $\Listener_0(\target \| \utt, \context; \theta)$, where $\theta$ represents the
vector of parameters that define the trained model.

\subsection{Base speaker}\label{sec:s0}

In order to avoid enumerating the exponentially or infinitely large space of possible utterances,
we also employ a probabilistic speaker model
$\Speaker_0(\utt \| \target, \context; \phi)$ to generate sets of alternative
utterances for each context.

The speaker model consists of an LSTM context encoder
and an LSTM description decoder (\figref{fig:model:speaker}). In this model, the colors of the context
$\referent_i \in \context$ are transformed into Fourier representation space,
and the sequence of color representations is passed through an LSTM with
100-dimensional hidden state. The context is reordered to place the target color
last, minimizing the length of dependence between the most important input color
and the output \cite{Sutskever2014} and eliminating the need to represent the
index of the target separately.
The output of this recurrent neural network is concatenated with a
100-dimensional embedding for the previous token at each time step in decoding.
The resulting vector is input along with the previous cell state to the LSTM cell,
and an affine transformation and softmax function are applied to the output to
produce a probability distribution predicting the following token of the description.
The model is substantively similar to well-known models for image caption generation
\cite{Karpathy2015,Vinyals2015}, which use the output of a convolutional neural
network as the representation of an input image and provide this representation
to the RNN as an initial state or first word (we represent the context using
a second RNN and concatenate the context representation onto each input word vector).

\subsection{Pragmatic listener}\label{sec:l2}

Using the above base agents, we define a pragmatic speaker
$\Speaker_{1}$ and a pragmatic listener
$\Listener_{1}$:
%
\begin{align}
\Speaker_1(\utt \| \target, \context; \theta)
  &= \frac{\Listener_0(\target \| \utt, \context; \theta)^\alpha}{\sum_{\utt'}
    \Listener_0(\target \| \utt', \context; \theta)^\alpha}
    \label{eq:s1} \\
  \Listener_1(\target \| \utt, \context; \theta)
  &=
    \frac{
    \Speaker_1(\utt \| \target, \context; \theta)
    }{
    \sum_{\target'} \Speaker_1(\utt \| \target', \context; \theta)
    }
\end{align}

These definitions mirror those in \eq{eq:rsa-s1} and \eq{eq:rsa-l1}
above, with $\Lex$ replaced by the learned weights $\theta$. We also
explicitly represent the context $\context$, because we can now define
features that explicitly take that information into account.
\todocheck{Could say more about this if there is room.}

Just as in \eqref{eq:rsa-s1}, the denominator in \eqref{eq:s1} should consist of a sum over
the entire set of potential utterances, which is exponentially large in the
maximum utterance length and might not even be finite.
As mentioned in \secref{sec:s0}, we limit this search by
taking $\numsamples$ samples from $\Speaker_0(\utt \| i, \context; \phi)$ for
each possible target index $i$, adding the actual utterance from the testing example,
and taking the resulting set as the universe of possible utterances. Taking a number
of samples from $\Speaker_0$ for each referent in the context gives the pragmatic
listener a variety of informative alternative utterances to consider when
interpreting the true input description. In practice, we have found that $\numsamples$
can be quite small; in our experiments, it is set to $8$.

To reduce the noise
resulting from the stochastically chosen alternative utterance sets, we also perform
this alternative-set sampling $n$ times and average the resulting probabilities in
the final $\Listener_1$ output. We again choose $n = 8$ as a satisfactory
compromise between effectiveness and computation time.

We find we can obtain an additional small improvement in listener accuracy
by blending the pragmatic listener with the base listener. This blended listener,
denoted $\Listener^*$, is defined as:
\begin{multline}
\Listener^{*}(\target \| \utt, \context; \theta) \propto \\ {\Listener_0}(\target \| \utt, \context; \theta)^{\beta} \cdot {}
\Listener_1(\target \| \utt, \context; \theta)^{1-\beta}  \label{eq:beta}
\end{multline}
%
This allows the model to adjust
the extent to which it overrides the learned model with the pragmatic
reasoning. The value of the base weight
$\beta$ can be any real number; however, we find that good values of $\beta$
lie in the range [-1, 1]. Setting $\beta = 0$ makes the blended model
equivalent to the pragmatic model; $\beta = 1$ ignores the
pragmatic reasoning and uses the base model's outputs; $\beta = -1$
``subtracts'' the base model from the pragmatic model (in log probability space)
to yield a ``hyperpragmatic'' model.

\subsection{Training} \label{sec:training}

We split our corpus into equal train/dev/test splits (16,635 contexts per split)
and preprocess the data by (1) lowercasing; (2) tokenizing
by splitting off punctuation as well as the endings \word{\mbox{-er}}, \word{\mbox{-est}}, and
\word{\mbox{-ish}};\footnote{We
only apply this heuristic ending segmentation for the listener; the speaker is trained to produce
words with these endings unsegmented.} and
(3) replacing tokens that appear once or not at all
in the training split with \texttt{<unk>}. We also remove
listener utterances and concatenate speaker utterances on the same context.
We leave handling of interactive dialogue to future work (\secref{sec:conclusion}).

We use ADADELTA
\cite{Zeiler2012} and Adam \cite{Kingma2014}, adaptive variants of
stochastic gradient descent (SGD), to train listener and speaker models.
The choice of optimization
algorithm and learning rate for each model were tuned with grid search
on a held-out tuning set consisting of 3,500 contexts.\footnote{For
  $\Listener_0$: ADADELTA, learning rate $\eta = {}$0.2; for
  $\Speaker_0$: Adam, learning rate $\alpha = {}$0.004.}

We also use a more fine-grained grid search on this tuning set to determine the
values of the two pragmatic reasoning parameters, $\alpha$ and $\beta$.
Our final model uses
$\alpha = 0.544$ and $\beta = -0.15$; see \secref{sec:alpha_beta} for more
discussion of these values.

% latex table generated in R 3.3.2 by xtable 1.8-2 package
% Thu Dec 15 15:35:44 2016
\begin{table*}[ht]
\centering
\begin{tabular}{lrrr@{\hspace{40pt}}rrr}
  \toprule
  & \multicolumn{3}{c}{human} & \multicolumn{3}{c}{pragmatic}\\
  & far& split& close& far& split& close\\
  \midrule
  \# Chars & 7.61 & 11.55 & 13.37 & 8.56 & 11.78 & 14.96 \\ 
  \# Words & \todocheck{1.46} & \todocheck{2.04} & \todocheck{2.27} & 1.62 & 2.13 & 2.62 \\ 
  \% Comparatives & 0.02 & 0.15 & 0.13 & 0.04 & 0.09 & 0.14 \\ 
  \% Superlatives & 0.02 & 0.06 & 0.16 & 0.05 & 0.10 & 0.17 \\ 
  \% Negatives & 0.03 & 0.10 & 0.12 & 0.04 & 0.09 & 0.14 \\ 
  \% High Specificity & 1.07 & 1.28 & 1.29 & 0.94 & 1.14 & 1.09 \\
  \bottomrule
\end{tabular}
\caption{Corpus statistics and statistics of samples from $\Speaker_1$
  (rates per utterance). The human and artificial speakers show
  the same correlations between language use and context type.
 \todocheck{todo: check human words rates, put errors in?}}
\label{table:metrics}
\end{table*}


\section{Behavioral results}

Our corpus was developed, not only to facilitate the development of
models for grounded language understanding, but also to provide a
richer picture of human pragmatic communication. The collection effort
was thus structured like a large-scale behavioral experiment, closely
following experimental designs like those of
\newcite{Clark:Wilkes-Gibbs:1986}. This paves the way to assessing our
computational model not only in its terms, but also in terms of how
its behavior compares to that of our human participants. Thus, the
current section briefly reviews some findings from the human
corpus that we use to inform our model assessment.

\todocheck{Robert could perhaps expand or improve this paragraph. Also,
are we doing enough with these humans results when we move to our model.
I fear we aren't!}

\begin{comment}
\begin{figure}
\includegraphics[scale = .5]{figures/allListenerAccuracy.eps}
\caption{Listener performance across conditions, for both human participants and model output}
\label{fig:listenerAccuracy}
\end{figure}
\end{comment}

\subsection{Listener behavior}

Since color reference is a difficult task even for humans, we compared listener accuracy across conditions to calibrate our expectations about model performance. While participants' accuracy was close to ceiling (97\%) on the \cond{far} condition, they made significantly more errors on the \cond{split} (90\%) and \cond{close} (83\%) conditions (see \todocheck{[make new figure/table for this]}).

\subsection{Speaker behavior}

For ease of comparison to computational results, we focus on five
metrics capturing different aspects of the rich pragmatic behavior
displayed by both human and artificial speakers in our task
(\tabref{table:metrics}).

\paragraph{Words and characters}
We expect human speakers to be verbose in \cond{close} contexts; even if
they do know enough basic color terms to distinguish all the colors
lexically, they might be unsure that their listeners will and so
resort to lengthy modifier phrases anyway. To assess this hypothesis,
we counted the average number of words and characters per message,
after removing stop words and punctuation. We found that participants
used longer messages when distractors were closer to the target in
color space. \todocheck{Could this be substantiated with a linear
  regression?} As \tabref{table:metrics} shows, our $\Speaker_{1}$ agent
shows exactly the same positive association between utterance length
and the \cond{far}, \cond{split}, and \cond{close} contexts. \todocheck{Regression
  for this too?}

\paragraph{Comparatives and superlatives}
As noted in \secref{sec:intro} comparative morphology implicitly
encodes a dependence on the context; a speaker who refers to the
target color as \word{the darker blue} is presupposing that there is
another (lighter) blue in the context. Similarly, superlatives like
\word{the bluest one} or \word{the lightest one} presuppose that all
the colors can be compared along a specific semantic dimension. We
thus expect to see this morphology more often where two or more of the
colors are comparable in this way. To test this, we used the Stanford
CoreNLP part-of-speech tagger \cite{Toutanova2003} to count the number
of comparatives (JJR or RBR) and superlatives (JJS or RBS) per
message. We found that participants were more likely to use these
constructions when one or more distractors were close to the target.
Additionally, we found evidence of an asymmetry in the use of these
constructions across the \cond{split} and \cond{close} conditions.
Comparatives were used somewhat more often in the \cond{split} condition,
where only one distractor was close to the target, while superlatives
were much more likely to be used in the \cond{close} condition.
\todocheck{Could this be substantiated with a linear regression?}

Importantly, our pragmatic speaker shows these same patterns,
producing more comparatives and superlatives in `close` conditions
than in the others \todocheck{regression for this too?}, suggesting
that it is simulating the human strategy at some level.

\paragraph{Negatives}
In our referential contexts, negation is likely to play a role similar
to that of comparatives: a phrase like \word{not the red or blue one}
singles out the third color, and \word{blue but not bright blue}
achieves a more nuanced kind of comparison. Thus, as with
comparatives, we expect negation to be more likely where two or more
distractors are close to the target. To test this, we counted
occurrences of the string `not' (by far the most frequent negation in
the corpus). We found that participants were more likely to use
negative constructions when one or more distractors were close to the
target. \todocheck{Could this be substantiated with a linear
  regression?} Our pragmatic speaker's use of negation shows the 
same relationship to the context \todocheck{regression summary?}.

\paragraph{WordNet specificity}
We expect speakers to prefer basic color terms wherever they suffice
to achieve the communicative goal, since such terms are most likely to
succeed with the widest range of listeners. Thus, a speaker might
choose \word{blue} even for a clear periwinkle color. However, as the
colors get closer together, the basic terms become too ambiguous, and
thus the risk of specific terms becomes worthwhile (though lengthy
descriptions might be a safer strategy, as discussed above). To
evaluate this idea, we use WordNet \cite{Fellbaum1998} to derive a
specificity hierarchy for color terms, and we hypothesized that
\cond{close} conditions will tend to lead speakers to go lower in this
hierarchy. \todocheck{To do... and include comparison with our $\Speaker_{1}$.}

\begin{table}[t]
\centering
\begin{tabular}{lrrrrrr}
  \toprule
   & \multicolumn{3}{c}{accuracy} & \multicolumn{3}{c}{perplexity} \\
   & $\Speaker_h$ & $\Speaker_0$ & $\Speaker_1$ & $\Speaker_h$ & $\Speaker_0$ & $\Speaker_1$ \\
  \midrule
  $\Listener_0$ & 83.39 & 83.66 & \oracle{99.75} & 1.69 & 1.71 & \oracle{1.03} \\
  $\Listener^*$ & \best{84.11} & \best{84.82} & \oracle{99.93} & \best{1.49} & \best{1.45} & \oracle{1.08}
  \\[0.5ex]
  $\Listener_h$ & 89.87 \\
   \bottomrule
\end{tabular}
\caption{Overall accuracy and perplexity of the base and pragmatic listeners
on inputs from various speakers, plus human accuracy. $\Speaker_h$ and $\Listener_h$
represent human agents. \oracle{Italics}: ``oracle'' results (see \secref{sec:speaker_eff}).}
\label{table:speakerVsListener}
\end{table}

\section{Model results}

\subsection{Listener accuracy}

The columns labeled $\Speaker_h$ in \tabref{table:speakerVsListener} show the accuracy
and perplexity of the base listener $\Listener_0$ and the pragmatic listener
$\Listener^*$ when interpreting the utterances of human speakers. We find that the
pragmatic model improves significantly\footnote{$p <{}$0.001, approximate
permutation test \cite{Pado2006}, 10,000 samples} over the base model on both metrics.

\begin{figure}
\centering
\includegraphics[scale = .45]{figures/changedByCondition.pdf}
\caption{Fraction of examples improved and declined, $\Listener^*$ over $\Listener_0$.}
\label{fig:changedByCondition}
\end{figure}

Breaking out the number of examples changed by condition
(\figref{fig:changedByCondition}) reveals that
the primary gain from the pragmatic model is in the \cond{close} condition, when the
model has to distinguish three colors that are all nearby in color space and
often cannot rely only on basic color terms.

\begin{figure}[t!]
\centering
\begin{tabular}{lr@{\hskip 5pt}r@{\hskip 5pt}r@{}r}
    \toprule
%     & \multicolumn{3}{c}{$\Listener_0$} \\
    $\Listener_0$ & \colorContext{3884C7}{02F9FD}{9E6461}{} \\
    \midrule
    \textbf{blue} &             9\% & \best{   91} & $<$1 
    \\[1ex]
    true blue     & \framebox{\gz{}11} & \best{     89} & $<$1 \\
    light blue    & \framebox{\zz{}$<$1} & \best{$>$99} & $<$1 \\
    brightest     & $<$1 & \framebox{\best{\z{}$>$99}} & $<$1 \\
    bright blue   & $<$1 & \framebox{\best{\z{}$>$99}} & $<$1 \\
    red           & $<$1 &     1 & \framebox{\best{\gz{}99}} \\
    purple        & $<$1 &     2 & \framebox{\best{\gz{}98}} \\
    \midrule
%   & \multicolumn{3}{c}{$\Speaker_1$} \\
    $\Speaker_1$ & \colorContext{3884C7}{02F9FD}{9E6461}{}  \\
    \midrule
    \textbf{blue} &             41  &    19 & $<$1
    \\[1ex]
    true blue     & \framebox{\best{\gz{}47}} &    19 & $<$1 \\
    light blue    & \framebox{\gzz{}5} & \best{   20} & $<$1 \\
    brightest     & $<$1 & \framebox{\best{\gz{}20}} & $<$1 \\
    bright blue   &    2 & \framebox{\best{\gz{}20}} & $<$1 \\
    red           &    1 &     2 & \framebox{\best{\gz{}50}} \\
    purple        &    5 &     1 & \framebox{\best{\gz{}50}} \\
    \midrule
%    & \multicolumn{3}{c}{$\Listener_1$} \\
    $\Listener_1$ & \colorContext{3884C7}{02F9FD}{9E6461}{} \\
    \midrule
    \textbf{blue} & \best{            68}  &    32 & $<$1 \\
    \midrule
%    true blue     & \framebox{\best{  72}} &    29 & $<$1 \\
%    light blue    & \framebox{  21} & \best{   79} & $<$1 \\
%    brightest     & $<$1 & \framebox{\best{$>$99}} & $<$1 \\
%    bright blue   &    7 & \framebox{\best{   93}} & $<$1 \\
%    red           &    2 &     5 & \framebox{\best{  94}} \\
%    purple        &    9 &     2 & \framebox{\best{  89}} \\
    $\Listener^*$ & \colorContext{3884C7}{02F9FD}{9E6461}{} \\
    \midrule
    \textbf{blue} & \best{            78}  &    22 & $<$1 \\
    \bottomrule
\end{tabular}
\caption{Conditional probability tables used to calculate $\Listener^*$
for one dev set example. Sample sizes are reduced to save space; here,
$m = 2$ and $n = 1$ (see \secref{sec:l2}).}
\label{fig:rsaExample}
\end{figure}

Examining the full probability tables for various dev set examples on which
$\Listener^*$ makes a difference reveals a general pattern. In most of these
examples, the alternative utterances sampled from $\Speaker_0$ for one of the
referents $i$ fail to identify
their intended referent to $\Listener_0$. The pragmatic listener interprets
this to mean that referent $i$ is inherently difficult to refer to,
and it compensates by increasing referent $i$'s probability. This is beneficial
when $i$ is the true target but harmful when $i$ is a distractor.

\Figref{fig:rsaExample}
shows one such example: a context consisting of a somewhat prototypical blue,
a bright cyan, and a purple-tinged brown, with the utterance \textit{blue}. The
base listener interprets this as referring to the cyan with 91\% probability,
perhaps due to the extreme saturation of the cyan maximally activating certain
parts of the neural network. However, when the pragmatic model takes samples
from $\Speaker_0$ to probe the space of alternative utterances, it becomes
apparent that communicating the more ordinary blue to the listener is difficult:
for the utterances chosen by the speaker intending this referent (\textit{true blue},
\textit{light blue}), the listener also
chooses the cyan with $>$89\% confidence.

Pragmatic reasoning overcomes this difficulty. Only two of the utterances in the
alternative set (the actual utterance \textit{blue} and the sampled alternative
\textit{true blue}) result in any appreciable probability mass on the true target,
so the pragmatic listener's model of the speaker predicts that the speaker
would choose between those utterances most of the time. However, if the target
were the cyan, the speaker would have a number of good options. Therefore, the
fact that the speaker chose \textit{blue} is interpreted as evidence for the
true target. This perfectly mirrors the back-and-forth reasoning behind the
definition of conversational implicature \cite{Grice75}.

\subsection{Speaker effectiveness} \label{sec:speaker_eff}

We also perform a ``machine communication'' experiment aimed at measuring the
ability of the speaker agents to convey the information necessary to identify
the target. \Tabref{table:speakerVsListener} shows the effectiveness of the
two speaker models---the base speaker $\Speaker_0$ (\secref{sec:s0}) and the
pragmatic speaker $\Speaker_1$ \eqref{eq:s1}---at describing the
target color to the listener models.

The pragmatic speaker is nearly perfect at communicating
with the listener models, which is unsurprising, because in the case of
$\Listener_0$, the speaker has access to the listener's thought process, and in
the case of $\Listener^*$, the listener has access to the speaker's thought process.
This gives the model the opportunity to optimize its output for test set
accuracy; hence, we label results on these pairs as \textit{oracle} accuracies.

Additionally, however, the listeners are able to understand even $\Speaker_0$'s
utterances better than humans'. This is surprising, because $\Listener_0$ does not
contain an embedded model of either speaker, and $\Listener^*$ uses $\Speaker_0$
only indirectly, as a source of alternative utterances to consider.

\subsection{Tuning the pragmatic parameters} \label{sec:alpha_beta}

As noted in \secref{sec:training}, we use an inverse temperature parameter of
$\alpha = 0.544$ and a base weight of $\beta = -0.15$ for the pragmatic
listener. Particularly surprising is the fact that the optimal value of $\beta$
discovered in grid search is \emph{negative}. This has the effect of amplifying
the difference between $\Listener_0$ and $\Listener^2$: the pragmatic model,
evidently, is not quite pragmatic enough.

The $\alpha$ value of 0.544${}<{}$1 indicates that the pragmatic listener is
modeling a somewhat less reliable speaker than one that uses a straightforward
softmax choice distribution. Empirical values of $\alpha$ fitting human behavior
in prior work with RSA-based models have varied from task to task;
\newcite{Kao-etal:2014} give $\alpha = 0.36$ for predicting
hyperbole in numeric expressions, while \newcite{Graf2016} find $\alpha = 10.8$ for
predicting hypernymy level in object references.

\begin{figure}
\centering
\includegraphics[width=\columnwidth]{figures/alpha_beta.eps}
\caption{Perplexity $\pi$ on tuning set as a function of $\alpha$ and $\beta$, plotted as $\log(\pi - \min_{\alpha,\beta} \pi(\alpha, \beta))$.}
\label{fig:alpha_beta}
\end{figure}

Grid search over values of $\alpha$ and $\beta$ revealed that the optimal values of
the two parameters are related (\figref{fig:alpha_beta}). Good combinations lay on
a curve with $\alpha$ decreasing as $\beta$ increased; however, negative $\beta$ and
very small $\alpha$ was catastrophic to perplexity.\footnote{We tuned the
parameters on perplexity rather than accuracy because accuracy was noisy and had
many local and global minima.}

\section{Related work}

Prior work combining machine learning with probabilistic pragmatic reasoning
models has largely focused on the speaker side, i.e., generation.
\newcite{Golland2010} develop a pragmatic speaker model,
$\Speaker(\Listener)$, that reasons about log-linear listeners trained on human
utterances containing spatial references in virtual-world environments.
\newcite{Tellex2014a} apply a similar technique, under the name
\term{inverse semantics}, to create a robot that can informatively ask
humans for assistance in accomplishing tasks. \newcite{Monroe2015} implement
an end-to-end trained $\Speaker(\Listener(\Speaker))$ model for referring
expression generation in a reference game task; their model requires enumerating
the set of possible utterances for each context, which is infeasible when
utterances are as varied as those in our dataset.

The closest work to ours that we are aware of is that of
\newcite{AndreasKlein16_NeuralPragmatics}, who also combine neural speaker
and listener models to model pragmatics. They implement and evaluate a
pragmatic speaker $\Speaker(\Listener_0)$, with sampling from a neural
$\Speaker_0$ model to limit the search space and regularize the model toward
human-like utterances. We show that these techniques prove beneficial in
understanding (listener) tasks as well. Approaching pragmatics from the listener
side requires taking the recursive reasoning process one step beyond theirs, to a
two-level derived pragmatic model $\Listener(\Speaker(\Listener_0))$; i.e., we
derive a pragmatic listener from their pragmatic speaker by normalizing once more
over the colors.

Additionally, we replace the multi-layer perceptron models over
$n$-gram features used in \newcite{AndreasKlein16_NeuralPragmatics}
with recurrent models for both the base listener and the base speaker.
These sequential models can approximate the compositional semantics
that is so crucial for operators like negation and comparatives. In
addition, our speaker is more expressive in that it is able to capture
properties of the context when sampling alternative utterances.
Finally, we add the $\alpha$ inverse temperature parameter from
\newcite{Goodman2013}, and we use a different strategy for blending
the pragmatic and base models: their model blends the base speaker and
base listener ($\Speaker_0^{\beta} \cdot \Listener_0^{1 - \beta}$) and
normalizes across utterances to implicitly define a different
pragmatic model, whereas we first compute the full normalized
pragmatic model ($\Listener(\Speaker(\Listener_0))$) and blend it with
a base listener by
$\Listener_0^{\beta} \cdot \Listener(\Speaker(\Listener_0))^{1 -
  \beta}$. \todocheck{This could be a liability; reviewers are likely
to ask how the two blending strategies compare on our task.}

\section{Conclusion} \label{sec:conclusion}

In this paper, we presented a newly-collected corpus of color descriptions from
reference games, and we showed that combining a model of pragmatic reasoning
with a neural color description understanding model results in more accurate
context-dependent description understanding compared with the neural model alone.

It is reasonable to expect that a model trained in an end-to-end fashion on
pragmatic data would be preferable to a pragmatic reasoning scheme applied on
top of a pre-trained base model.
Indeed, the probabilistic pragmatic work in the cognitive science literature
on which this work is based assumes the base listener represents literal semantics
and is not pragmatic at all; this suggests that our model relies on the base
listener failing to learn pragmatic behavior already present in the dataset.
An enticing avenue of future work is to use variational methods to scale
up an end-to-end pragmatic model like that of \newcite{Monroe2015} to
allow for arbitrarily large spaces of alternative utterances.

Another area of investigation we did not pursue in this work is the role of
multi-turn dialogues in the corpus we collected. As noted in \secref{sec:corpus},
both participants in our reference game task had the ability to use the chat window
at any point, and many \todocheck{(how many?)} took advantage of the ability
to have a two-way discussion. Modeling dialogue agents is substantially
more challenging than modeling isolated speakers and listeners; multi-turn dialogue
requires reasoning about previous utterances as context, anticipating
responses for long-term planning, and (for the listener) deciding when
to ask for clarification rather than commit to a referent
\cite{Clark:Wilkes-Gibbs:1986,Clark96}
\todocheck{(surely there are some more classical citations on this problem)}.
We look forward to examining this problem further, and we release our
dataset\footnote{\url{http://anonymized/}}
with the expectation that others may find interest in this aspect as well.


%\section{Credits}
%
%This document has been adapted from the instructions for ACL proceedings, including those for ACL-2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui, those for ACL-2005 by Hwee Tou Ng and Kemal Oflazer, those for ACL-2002 by Eugene Charniak and Dekang Lin, and earlier ACL and EACL formats. Those versions were written by several people, including John Chen, Henry S. Thompson and Donald Walker. Additional elements were taken from the formatting instructions of the {\em International Joint Conference on Artificial Intelligence}.
%
%\section{Introduction}
%
%The following instructions are directed to authors of papers submitted to TACL or accepted for publication. All authors are required to adhere to these specifications. Authors are required to provide a Portable Document Format (PDF)
%% das: removed reference to PostScript
%% and PostScript
%version of their papers. \textbf{The proceedings will be printed on
%US-Letter paper}. Authors from countries in which access to
%word-processing systems is limited should contact TACL editors at editors-in-chief@transacl.org as soon as possible.
%
%
%\section{General Instructions}
%
%Manuscripts must be in two-column format. Exceptions to the two-column format include the title,
%authors' names and complete addresses, which must be centered at the top of the first page,
%and any full-width figures or tables (see the guidelines in Subsection~\ref{ssec:first}). {\bf Type single-spaced}.
%Start all pages directly under the top margin. See the guide-lines later regarding formatting the first page. Do not number the pages.
%
%\subsection{Electronically-available resources}
%
%TACL provides this description in \LaTeX2e (tacl.tex) and PDF format (tacl.pdf), along with the LATEX2e style file used to format it (acl2012.sty) and an ACL bibliography style (acl2012.bst).  A Microsoft Word template file (tacl.dot) is also available. We require the use of these style files, which have been appropriately tailored for TACL. If you have an option, we recommend that you use the \LaTeX2e version. \textbf{If you will be using the Microsoft Word template, we suggest that you anonymize your source file so that the pdf produced does not retain your identity.} This can be done by removing any personal information from your source
%document properties.
%
%
%\subsection{Format of Electronic Manuscript}
%\label{sect:pdf}
%
%For the production of the electronic manuscript you must use Adobe's
%Portable Document Format (PDF). This format can be generated from
%postscript files: on Linux/Unix systems, you can use {\tt ps2pdf} for this
%purpose; under Microsoft Windows, you can use Adobe's Distiller, or
%if you have {\tt cygwin} installed, you can use {\tt dvipdf} or
%{\tt ps2pdf}.  Note
%that some word processing programs generate PDF which may not include
%all the necessary fonts (esp. tree diagrams, symbols). When you print
%or create the PDF file, there is usually an option in your printer
%setup to include none, all or just non-standard fonts.  Please make
%sure that you select the option of including ALL the fonts.  {\em Before sending it, test your PDF by printing it from a computer different from the one where it was created}. Moreover,
%some word processor may generate very large postscript/PDF files,
%where each page is rendered as an image. Such images may reproduce
%poorly.  In this case, try alternative ways to obtain the postscript
%and/or PDF.  One way on some systems is to install a driver for a
%postscript printer, send your document to the printer specifying
%``Output to a file'', then convert the file to PDF.
%
%Additionally, it is of utmost importance to specify the {\bf US-Letter format} (8.5in $\times$ 11in) when formatting the paper. When working with {\tt dvips}, for instance, one should specify {\tt -t letter}.
%
%Print-outs of the PDF file on US-Letter paper should be identical to the
%hardcopy version.  If you cannot meet the above requirements about the
%production of your electronic submission, please contact the
%publication chair above as soon as possible.
%
%
%\subsection{Layout}
%\label{ssec:layout}
%
%Format manuscripts two columns to a page, in the manner these
%instructions are formatted. The exact dimensions for a page on US-letter
%paper are:
%
%\begin{itemize}
%\item Left and right margins: 1in
%\item Top margin:1in
%\item Bottom margin: 1in
%\item Column width: 3.15in
%\item Column height: 9in
%\item Gap between columns: 0.2in
%\end{itemize}
%
%\noindent Papers should not be submitted on any other paper size. If you cannot meet the above requirements about the production of your electronic submission, please contact the publication chair above as soon as possible.
%
%\subsection{Fonts}
%
%For reasons of uniformity, Adobe's {\bf Times Roman} font should be
%used. In \LaTeX2e{} this is accomplished by putting
%
%\begin{quote}
%\begin{verbatim}
%\usepackage{times}
%\usepackage{latexsym}
%\end{verbatim}
%\end{quote}
%in the preamble. If Times Roman is unavailable, use {\bf Computer
%Modern Roman} (\LaTeX2e{}'s default).  Note that the latter is about
%10\% less dense than Adobe's Times Roman font.
%
%
%\begin{table}[h]
%\begin{center}
%\begin{tabular}{|l|rl|}
%\hline \bf Type of Text & \bf Font Size & \bf Style \\ \hline
%paper title & 15 pt & bold \\
%author names & 12 pt & bold \\
%author affiliation & 12 pt & \\
%the word ``Abstract'' & 12 pt & bold \\
%section titles & 12 pt & bold \\
%document text & 11 pt  &\\
%captions & 10 pt & \\
%abstract text & 10 pt & \\
%bibliography & 10 pt & \\
%footnotes & 9 pt & \\
%\hline
%\end{tabular}
%\end{center}
%\caption{\label{font-table} Font guide. }
%\end{table}
%
%\subsection{The First Page}
%\label{ssec:first}
%
%Center the title, author's name(s) and affiliation(s) across both
%columns. Do not use footnotes for affiliations.  Do not include the
%paper ID number assigned during the submission process.
%Use the two-column format only when you begin the abstract.
%
%{\bf Title}: Place the title centered at the top of the first page, in
%a 15 point bold font.  (For a complete guide to font sizes and styles, see Table~\ref{font-table}.)
%Long title should be typed on two lines without
%a blank line intervening. Approximately, put the title at 1in from the
%top of the page, followed by a blank line, then the author's names(s),
%and the affiliation on the following line.  Do not use only initials
%for given names (middle initials are allowed). Do not format surnames
%in all capitals (e.g., ``Zhou,'' not ``ZHOU'').  The affiliation should
%contain the author's complete address, and if possible an electronic
%mail address. Leave about 0.75in between the affiliation and the body
%of the first page. The title, author names and addresses should be completely identical to those entered to the electronic paper submission website in order to maintain the consistency of author information among all publications of the conference.
%
%{\bf Abstract}: Type the abstract at the beginning of the first
%column.  The width of the abstract text should be smaller than the
%width of the columns for the text in the body of the paper by about
%0.25in on each side.  Center the word {\bf Abstract} in a 12 point
%bold font above the body of the abstract. The abstract should be a
%concise summary of the general thesis and conclusions of the paper.
%It should be no longer than 200 words. The abstract text should be in 10 point font.
%
%{\bf Text}: Begin typing the main body of the text immediately after
%the abstract, observing the two-column format as shown in
%the present document. Do not include page numbers.
%
%{\bf Indent} when starting a new paragraph. For reasons of uniformity,
%use Adobe's {\bf Times Roman} fonts, with 11 points for text and
%subsection headings, 12 points for section headings and 15 points for
%the title.  If Times Roman is unavailable, use {\bf Computer Modern
%Roman} (\LaTeX2e's default; see section \ref{sect:pdf} above).
%Note that the latter is about 10\% less dense than Adobe's Times Roman
%font.
%
%\subsection{Sections}
%
%{\bf Headings}: Type and label section and subsection headings in the
%style shown on the present document.  Use numbered sections (Arabic
%numerals) in order to facilitate cross references. Number subsections
%with the section number and the subsection number separated by a dot,
%in Arabic numerals. Do not number subsubsections.
%
%{\bf Citations}: Citations within the text appear
%in parentheses as~\cite{Gusfield:97} or, if the author's name appears in
%the text itself, as Gusfield~\shortcite{Gusfield:97}. Append lowercase letters to the year in cases of ambiguities. Treat double authors as in~\cite{Aho:72}, but write as in~\cite{Chandra:81} when more than two authors are involved. Collapse multiple citations as in~\cite{Gusfield:97,Aho:72}. Also refrain from using full citations as sentence constituents. We suggest that instead of
%\begin{quote}
%``\cite{Gusfield:97} showed that ...''
%\end{quote}
%you use
%\begin{quote}
%``Gusfield \shortcite{Gusfield:97}   showed that ...''
%\end{quote}
%
%If you are using the provided \LaTeX{} and Bib\TeX{} style files, you
%can use the command \verb|\newcite| to get ``author (year)'' citations.
%
%As reviewing will be double-blind (except that action editors know author identity and authors know action-editor identity), the submitted version of the papers should not include the
%authors' names and affiliations. Furthermore, self-references that
%reveal the author's identity, e.g.,
%\begin{quote}
%``We previously showed \cite{Gusfield:97} ...''
%\end{quote}
%should be avoided. Instead, use citations such as
%\begin{quote}
%``Gusfield \shortcite{Gusfield:97}
%previously showed ... ''
%\end{quote}
%
%To be clear: You should reference your prior work if it is relevant; but use the third person instead of the 1st person and in place of references like “(Anonymous) showed…”, since such anonymized references do not allow readers to examine relevant related work.
%
%Authors’ names should also be removed from the ``Document Properties'' display that can be viewed using Adobe Acrobat’s ``File $\rightarrow$ Properties'' menu.
%
%Please do not include acknowledgements when submitting your papers. Papers that do not conform
%to these requirements may be rejected without review.
%
%\textbf{References}: Gather the full set of references together under
%the heading {\bf References}; place the section before any Appendices,
%unless they contain references. Arrange the references alphabetically
%by first author, rather than by order of occurrence in the text.
%Provide as complete a citation as possible, using a consistent format,
%such as the one for {\em Computational Linguistics\/} or the one in the
%{\em Publication Manual of the American
%Psychological Association\/}~\cite{APA:83}.  Use of full names for
%authors rather than initials is preferred.  A list of abbreviations
%for common computer science journals can be found in the ACM
%{\em Computing Reviews\/}~\cite{ACM:83}.
%
%The \LaTeX{} and Bib\TeX{} style files provided roughly fit the
%American Psychological Association format, allowing regular citations,
%short citations and multiple citations as described above.
%
%{\bf Appendices}: Appendices, if any, directly follow the text and the
%references (but see above).  Letter them in sequence and provide an
%informative title: {\bf Appendix A. Title of Appendix}.
%
%\textbf{Acknowledgment} sections should go as a last section immediately
%before the references. Do not number the acknowledgement section.
%
%\subsection{Footnotes}
%
%{\bf Footnotes}: Put footnotes at the bottom of the page. They may
%be numbered or referred to by asterisks or other
%symbols.\footnote{This is how a footnote should appear.} Footnotes
%should be separated from the text by a line.\footnote{Note the
%line separating the footnotes from the text.}  Footnotes should be in 9 point font.
%
%\subsection{Graphics}
%
%{\bf Illustrations}: Place figures, tables, and photographs in the
%paper near where they are first discussed, rather than at the end, if
%possible.  Wide illustrations may run across both columns and should be placed at
%the top of a page. Color illustrations are discouraged, unless you have verified that
%they will be understandable when printed in black ink.
%
%{\bf Captions}: Provide a caption for every illustration; number each one
%sequentially in the form:  ``Figure 1. Caption of the Figure.'' ``Table 1.
%Caption of the Table.''  Type the captions of the figures and
%tables below the body, using 10 point text.
%
%\section{Translation of non-English Terms}
%
%It is also advised to supplement non-English characters and terms
%with appropriate transliterations and/or translations
%since not all readers understand all such characters and terms.
%
%Inline transliteration or translation can be represented in
%the order of: original-form transliteration ``translation''.
%
%\section{Length of Submission}
%\label{sec:length}
%
%Submissions may consist of seven to ten (7-10) letter format (not A4) pages of content and unlimited additional pages (only) allowed for references. Papers that are revisions of submissions with prior (b) or (c) decisions may be allowed one to two additional pages of content to accommodate required revisions.
%
%Appendices (if any) are counted as content pages. Papers that do not conform to the specified length and
%formatting requirements are subject to re-submission.
%
%
%\section*{Acknowledgments}
%
%Do not number the acknowledgment section. Do not include this section when submitting your paper for review.

\bibliography{colors}
\bibliographystyle{acl2012}

\end{document}
