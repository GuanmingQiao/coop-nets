%
% File acl2012.tex
%
% Contact: Maggie Li (cswjli@comp.polyu.edu.hk), Michael White (mwhite@ling.osu.edu)
%%
%% Based on the style files for ACL2008 by Joakim Nivre and Noah Smith
%% and that of ACL2010 by Jing-Shin Chang and Philipp Koehn


\documentclass[11pt,letterpaper]{article}
\usepackage[letterpaper]{geometry}
\usepackage{acl2012}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{todonotes}
\usepackage{url}
\usepackage{color}
\usepackage[inline]{enumitem}
\usepackage{booktabs}
\usepackage[font=small]{caption}
\usepackage[font=small]{subcaption}
\usepackage{colortbl}
\usepackage{tikz}
\usepackage{comment}
\usepackage{epstopdf}
\usetikzlibrary{shapes.geometric}
\makeatletter
\newcommand{\@BIBLABEL}{\@emptybiblabel}
\newcommand{\@emptybiblabel}[1]{}
\makeatother
\usepackage[hidelinks,draft]{hyperref}
\DeclareMathOperator*{\argmax}{arg\,max}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Code to use with NAACL/ACL style files to simulate natbib's
% \citealt, which prints citations with no parentheses. This should
% work if pasted into the preamble. \cite, \newcite, and \shortcite
% should continue to work as before.

\makeatletter

\def\citealt{\def\citename##1{{\frenchspacing##1} }\@internalcitec}

\def\@citexc[#1]#2{\if@filesw\immediate\write\@auxout{\string\citation{#2}}\fi
  \def\@citea{}\@citealt{\@for\@citeb:=#2\do
    {\@citea\def\@citea{;\penalty\@m\ }\@ifundefined
       {b@\@citeb}{{\bf ?}\@warning
       {Citation `\@citeb' on page \thepage \space undefined}}%
{\csname b@\@citeb\endcsname}}}{#1}}

\def\@internalcitec{\@ifnextchar [{\@tempswatrue\@citexc}{\@tempswafalse\@citexc[]}}

\def\@citealt#1#2{{#1\if@tempswa, #2\fi}}

\makeatother

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newcommand{\term}{\textit}

\newcommand{\word}{\textit}

\newcommand{\eq}[1]{(\ref{#1})}

\newcommand{\Listener}{L}
\newcommand{\Speaker}{S}
\newcommand{\utt}{u}
\newcommand{\uttlen}{N}
\newcommand{\referent}{c}
\newcommand{\context}{C}
\newcommand{\contextlen}{K}
\newcommand{\target}{t}
\newcommand{\numsamples}{m}
\newcommand{\feat}{f}
\renewcommand{\|}{\mid}
\newcommand{\best}[1]{\textbf{#1}}
\newcommand{\oracle}[1]{\textit{#1}}

\newcommand{\set}[1]{\left\{#1\right\}}

\newcommand{\Secref}[1]{Section~\ref{#1}}
\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\dashsecref}[2]{Sections~\ref{#1}--\ref{#2}}
\newcommand{\Figref}[1]{Figure~\ref{#1}}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\dashfigref}[2]{Figures~\ref{#1}--\ref{#2}}
\newcommand{\Tabref}[1]{Table~\ref{#1}}
\newcommand{\tabref}[1]{Table~\ref{#1}}
\newcommand{\pararef}[1]{\textbf{#1}}

\definecolor{ourlightblue}{HTML}{03A9F4}
\definecolor{ourgreen}{HTML}{4D8951}
\definecolor{oursteelblue}{HTML}{9BB8D7}
\definecolor{ourorange}{HTML}{FDBA58}

\newcommand{\ourlightblue}[1]{{\color{ourlightblue}#1}}
\newcommand{\ourgreen}[1]{{\color{ourgreen}#1}}
\newcommand{\oursteelblue}[1]{{\color{oursteelblue}#1}}
\newcommand{\ourorange}[1]{{\color{ourorange}#1}}

\newcolumntype{P}[1]{>{\raggedright\arraybackslash}p{#1}}

\newcommand{\colorPatch}[2][xxxx]{
  \colorbox[HTML]{#2}{{\color[HTML]{#2}#1}}}

\newcommand{\colorSolo}[2]{
  \negthickspace\colorPatch{#1} & #2}

\newcommand{\colorContext}[4]{
  \framebox{\negthickspace\colorPatch{#1}} & \colorPatch{#2} & \colorPatch{#3} & #4}


\newcommand{\colorContextGame}[3]{
  \negthickspace\colorPatch{#1} & \colorPatch{#2} & \colorPatch{#3}}

\newcommand{\colorContextCompact}[3]{
  \colorPatch[xx]{#1} & \colorPatch[xx]{#2} & \colorPatch[xx]{#3}}

\newcommand{\colorContextNarrow}[3]{
  \colorPatch[xx]{#1}, \colorPatch[xx]{#2}, \colorPatch[xx]{#3}}

\newcommand{\todocheck}[1]{\textcolor{red}{#1}}
% For filling out boxed table cells to equal length
\newcommand{\gzz}{\phantom{$<$00}}
\newcommand{\gz}{\phantom{$<$0}}
\newcommand{\zz}{\phantom{00}}
\newcommand{\z}{\phantom{0}}
%\newcommand{\p}{\phantom{\%}}
\newcommand{\p}{}

 \definecolor{Green}{RGB}{10,200,100}
\newcommand{\ndg}[1]{\textcolor{Green}{[ndg: #1]}}  

\newcommand{\cond}{\emph}

\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{Colors in Context: A Pragmatic Neural Model for \\
Grounded Language Understanding}

\author{}
% First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   {\tt email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}


We present a model of pragmatic referring expression interpretation
 in a grounded communication task
(identifying colors from descriptions) that draws upon predictions
from two recurrent neural network classifiers, a speaker and a listener,
unified by a recursive pragmatic reasoning framework.
Experiments show that this
combined pragmatic model interprets color descriptions 
more accurately than the classifiers from which it is built.
We observe that pragmatic reasoning helps primarily
in the hardest cases: when the model must distinguish very similar colors,
or when few utterances adequately express
the target color.
Our findings make use of a newly-collected corpus of human utterances in
color reference games, which exhibit a variety of pragmatic behaviors;
we also show that the embedded speaker model reproduces many of these
 pragmatic effects.

\end{abstract}

\section{Introduction} \label{sec:intro}

% Grounded language use requires adapting language to the context at hand: trying
% to identify an object by describing it as ``blue'' will be unhelpful if all
% accessible objects are some shade of blue. People assume their interlocutors know
% this and adapt their interpretation of grounded language accordingly. This intuition
% is captured in game-theoretic and probabilistic models of pragmatic
% language understanding \cite{Jaeger2012,Frank2012}.

Human communication is \emph{situated}. In using language, we are sensitive
to context and our interlocutors' expectations, when choosing our
utterances (as speakers) and when interpreting the utterances we hear (as listeners).
Visual referring tasks exercise this complex process of grounding, 
in the environment and in
our mental models of each other, and thus provide a valuable test-bed for
computational models of production and comprehension.

\begin{table}
  \centering
  \setlength{\tabcolsep}{4pt}
  \begin{tabular}[c]{r@{. \ } ccc l}
    \toprule
    \multicolumn{4}{c}{Context} & Utterance \\
    \midrule
    1&\colorContext{2421DE}{605DA2}{0144FE}{darker blue}\\
    2&\colorContext{5866A7}{2DD2BC}{C23D5A}{Purple}\\
    3&\colorContext{5866A7}{9953AC}{2DD2A6}{blue}\\
    4&\colorContext{3884C7}{02F9FD}{9E6461}{blue}\\
    \bottomrule
  \end{tabular}
  \caption{Examples of color reference in context, taken from our corpus. The target color
    is boxed. The speaker's description is shaped not only
    by this target, but also by the other context colors and their
    relationships.}
  \label{table:examples}
\end{table}

\Tabref{table:examples} illustrates the situated nature of
reference understanding with descriptions of colors from a task-oriented
dialogue corpus we
introduce in this paper. In these dialogues, the speaker is trying
to identify their (privately assigned) target color for the
listener. In context~1, the comparative \word{darker} implicitly
refers to both the target (boxed) and one of the other colors. In
contexts 2 and 3, the target color is the same, but the distractors
led the speaker to choose different basic color terms. In
context~4, \word{blue} is a pragmatic choice even though two colors are
shades of blue, because the interlocutors assume about each other that
they find the target color a more prototypical representative of blue
and would prefer other descriptions (\word{teal}, \word{cyan}) for the middle color.
The fact that \word{blue} is the utterance of choice in three of these
four cases highlights the context dependence of color descriptions.

In this paper, we present a scalable, learned model of pragmatic
language understanding. At its core, the model is a version of the
Rational Speech Acts (RSA) model \cite{Frank2012,GoodmanFrank16_RSATiCS}, in which agents
reason recursively about each other's expectations and intentions to
communicate more effectively than literal semantic agents could. In
most work on RSA, the literal semantic agents use fixed message sets
and stipulated grammars, which is a barrier to experiments in
linguistically complex domains. In our formulation, the literal
semantic agents are recurrent neural networks (RNNs) that produce and
interpret color descriptions in context. These models are learned from
data and scale easily to large data sets containing novel utterances.
The RSA recursion is then defined in terms of these base agents: the
\emph{pragmatic speaker} produces utterances based on a literal RNN
listener \cite{AndreasKlein16_NeuralPragmatics}, and the
\emph{pragmatic listener} interprets utterances based on the pragmatic
speaker's behavior.

% We evaluate our model in a \term{reference game} task: given a context consisting
% of a set of potential \term{referents}, one referent is selected as the \term{target}
% and its identity indicated secretly to the \term{speaker}. The speaker then must
% convey the identity of the target to a \term{listener}. Our model plays the
% role of the listener in this game, attempting to resolve a human utterance
% to the correct referent. The reference game setting tests the ability of our model
% to do two distinct forms of pragmatic reasoning: the model must consider
% not only world context in the form of the set of available referents,
% but also linguistic context in the form of the speaker's goals and constraints in
% describing the target.

We evaluate this model with a new, psycholinguistically motivated corpus of real-time, dyadic reference games in which
the referents are patches of color. 
The corpus includes 942 complete games with 47,100 utterances produced by
human participants paired into dyads on the web. The linguistic
behavior of the players exhibits many of the intricacies of language
in general, including not just the context dependence and cognitive
complexity discussed above, but also compositionality, vagueness, and
ambiguity. While many previous data sets feature descriptions of
individual colors \cite{Cook2005,Munroe2010,Kawakami2016}, situating
colors in context elicits greater variety in language use,
including negations, comparatives, superlatives,
metaphor, and shared associations.

In this paper we focus on accuracy in a listener task
(i.e., at language understanding).
However, our most successful model integrates speaker and listener perspectives,
combining predictions made by a system trained to understand color descriptions
and one trained to produce them.
Experiments on the data in our corpus show that this combined pragmatic model
improves accuracy
in interpreting human-produced descriptions over the basic RNN listener
alone. Moreover, the improvements from pragmatic reasoning come primarily
in the hardest cases:
%
\begin{enumerate*}[label=(\arabic*)]
\item contexts with colors that are very similar, thus requiring
  the interpretation of descriptions that convey fine distinctions;
  and
\item target colors that most referring expressions fail to identify,
  whether due to a lack of adequate descriptive terms or a consistent
  bias against the color in the RNN listener.
\end{enumerate*}


\section{Task and data collection}\label{sec:corpus}

We evaluate our agents on a task of language understanding in a dyadic reference
game \cite{Rosenberg:Cohen:1964,KraussWeinheimer64_ReferencePhrases,Paetzel-etal:2014}. Unlike traditional natural language processing tasks, in which participants provide impartial judgements of language in isolation, reference games embed language use in a goal-oriented communicative context \cite{Clark96,TanenhausBrownSchmidt08_LanguageNatural}. Since they offer the simplest experimental setup where many pragmatic and discourse-level phenomena emerge, these games have been used widely in cognitive science to study topics like common ground and conventionalization \cite{Clark:Wilkes-Gibbs:1986}, referential domains \cite{BrownSchmidtTanenhaus08_TargetedGame}, perspective-taking \cite{HannaTanenhausTrueswell03_CommonGroundPerspective}, and overinformativeness \cite{KoolenGattGoudbeekKrahmer11_Overspecification}.

%More formally, in our reference game, a listener agent $\Listener$ is presented with a context set of colors
%$\context = \{\referent_{i}, i=1..\contextlen\}$ and an English
%utterance $\utt$ that describes the target color
%$\referent_{\target} \in \context$. The agent does not know the index
%$\target$ of the target color and must guess it from $\utt$.

\begin{figure}
\includegraphics[scale = .2]{figures/speakerView.png}
\caption{Example trial in corpus collection task, from speaker's
  perspective. The target color (boxed) was presented among two distractors on a neutral background.}
\label{fig:taskScreenshot}
\end{figure}
To obtain a corpus of natural color reference data across varying
contexts, we recruited 967 unique participants from Amazon
Mechanical Turk to play 1,059 games of 50 rounds each, using the open-source framework of
\newcite{Hawkins15_RealTimeWebExperiments}.  
Participants were sorted into dyads, randomly assigned the role of speaker or listener,
and placed in a game environment containing a chat box and an array of three color patches
(\figref{fig:taskScreenshot}). 
On each round, one of the three colors was chosen to be the target and
highlighted for the speaker. They were instructed to communicate this
information to the listener, who could then click on one of the colors
to advance to the next trial. Both participants were free to use the
chat box at any point.


To ensure a range of difficulty, we randomly interspersed an equal
number of trials from three different conditions:
%
\begin{enumerate*}[label=(\arabic*)]%
\item \cond{close}, where colors were all within a distance of
  $\theta$ from one another but still perceptible,\footnote{We used the
    most recent CIEDE standard to measure color differences, which is
    calibrated to human vision \cite{SharmaWuDalal05_DeltaE}. All
    distances were constrained to be larger than a lower bound of
    $\epsilon = 5$ to ensure perceptible differences, and we used a
    threshold value of $\theta = 20$ to create conditions.} 
\item \cond{split}, where one distractor was within a distance of
  $\theta$ of the target, but the other distractor was farther than
  $\theta$, and
\item \cond{far}, where all colors were farther than $\theta$ from one
  another. Colors were rejection sampled uniformly from RGB (red,
  green, blue) space to meet these constraints.
\end{enumerate*}

After excluding extremely long messages,\footnote{Specifically, 627 messages with greater than $4\sigma$ of the mean number of words per message; these often included meta-commentary about the game rather than color terms.} incomplete games, and games whose participants self-reported confusion about the instructions or non-native English proficiency, we were left with a corpus of 53,365 speaker utterances across 46,994 rounds in 948 games. The three conditions are equally represented, with 15,519 \emph{close} trials, 15,693 \emph{split} trials, and 15,782 \emph{far} trials. Participants were allowed to play more than once, but the modal number of games played per participant was one (75\%). The modal number of messages sent per round was also one (90\%). We release the raw, pre-filter data from these experiments alongside the filtered corpus we used throughout our analyses (see Footnote
\ref{foot:release}).

\section{Behavioral results}

Our corpus was developed not only to facilitate the development of
models for grounded language understanding, but also to provide a
richer picture of human pragmatic communication. The collection effort
was thus structured like a large-scale behavioral experiment, closely
following experimental designs like those of
\newcite{Clark:Wilkes-Gibbs:1986}. This paves the way to assessing our
model not solely based on the listener's classification accuracy, but also in terms of how
qualitative features of the speaker's production compare to that of our human participants. 
Thus, the current section briefly reviews some novel findings from the human
corpus that we use to inform our model assessment.

\subsection{Listener behavior}

Since color reference is a difficult task even for humans, we compared listener accuracy across conditions to calibrate our expectations about model performance. While participants' accuracy was close to ceiling (97\%) on the \cond{far} condition, they made significantly more errors on the \cond{split} (90\%) and \cond{close} (83\%) conditions (see \figref{fig:listenerAccuracy}).

% latex table generated in R 3.3.2 by xtable 1.8-2 package
% Mon Jan  9 13:58:36 2017
\begin{table*}[ht]
\centering
\begin{tabular}{lrrrr@{\hspace{20pt}}rrrr@{\hspace{20pt}}rrr}
  \toprule
  & \multicolumn{3}{c}{human}&& \multicolumn{3}{c}{$\Speaker_0$}&& \multicolumn{3}{c}{$\Speaker_1$}\\
  & far& split& close&& far& split& close&& far& split& close\\ \midrule
  \# Chars            & 7.8 & 12.3 & 14.9 && 9.0 & 12.8 & 16.6 && 9.0 & 12.8 & 16.4 \\ 
  \# Words            & 1.7 & 2.7 & 3.3   && 2.0 & 2.8 & 3.7   && 2.0 & 2.8 & 3.7   \\ 
  \% Comparatives     & 1.7 & 14.2 & 12.8 && 3.6 & 8.8 & 13.1  && 4.2 & 9.0 & 13.7  \\ 
  \% High Specificity & 7.0 & 7.6 & 7.4   && 6.4 & 8.4 & 7.6   && 6.8 & 7.9 & 7.5   \\ 
  \% Negatives        & 2.8 & 10.0 & 12.9 && 4.8 & 8.9 & 13.3  && 4.4 & 8.5 & 14.1  \\ 
  \% Superlatives     & 2.2 & 6.1 & 16.7  && 4.7 & 9.7 & 17.2  && 4.8 & 10.3 & 16.6 \\ 
   \bottomrule
\end{tabular}
\caption{Corpus statistics and statistics of samples from artificial speakers defined
  in \secref{sec:l2} (rates per utterance). The human and artificial speakers show
  many of the same correlations between language use and context type.} 
\label{table:metrics}
\end{table*}

\subsection{Speaker behavior} \label{sec:speaker_behavior}

For ease of comparison to computational results, we focus on five
metrics capturing different aspects of pragmatic behavior
displayed by both human and artificial speakers in our task
(\tabref{table:metrics}). In all cases, we report test statistics from a mixed-effects regression including condition as a fixed effect and game ID as a random effect; except where noted, all test statistics reported correspond to $p$-values${}<10^{-4}$ and have been omitted for readability.

\paragraph{Words and characters}
We expect human speakers to be more verbose in \cond{split} and \cond{close} 
contexts than \cond{far} contexts; the shortest, simplest color terms for the target 
may also apply to one or both distractors, thus incentivizing the speaker to use more
lengthy descriptions to fully distinguish it. Indeed, even if they \emph{know} 
enough simple color terms to distinguish all the colors
lexically, they might be unsure their listeners will and so
resort to modifiers anyway. To assess this hypothesis,
we counted the average number of words and characters per message. 
Compared to the baseline \cond{far} context, participants used significantly longer messages both in the \cond{split} context ($t =  45.85$) and the \cond{close} context ($t = 73.06$). Similar results hold for the character metric.

\paragraph{Comparatives and superlatives}
As noted in \secref{sec:intro} comparative morphology implicitly
encodes a dependence on the context; a speaker who refers to the
target color as \word{the darker blue} is presupposing that there is
another (lighter) blue in the context. Similarly, superlatives like
\word{the bluest one} or \word{the lightest one} presuppose that all
the colors can be compared along a specific semantic dimension. We
thus expect to see this morphology more often where two or more of the
colors are comparable in this way. To test this, we used the Stanford
CoreNLP part-of-speech tagger \cite{Toutanova2003} to mark the presence or absence of comparatives (JJR or RBR) and superlatives (JJS or RBS) for each message. 

We found two related patterns across conditions. First, participants were significantly 
more likely to use both comparatives ($z = 37.39$) and superlatives ($z = 31.32$) 
when one or more distractors were close to the target. Second, we found evidence of 
an asymmetry in  the use of these constructions across the \cond{split} and 
\cond{close} contexts. Comparatives were used significantly more often in the 
\cond{split} context ($z = 4.4$), where only one distractor was close to the target, 
while superlatives were much more likely to be used in the \cond{close} condition 
($z = 32.72$).\footnote{We used Helmert coding to examine this pair of orthogonal 
contrasts, compared to dummy coding in the analysis.}

\paragraph{Negatives}
In our referential contexts, negation is likely to play a role similar
to that of comparatives: a phrase like \word{not the red or blue one}
singles out the third color, and \word{blue but not bright blue}
achieves a more nuanced kind of comparison. Thus, as with
comparatives, we expect negation to be more likely where one or more
distractors are close to the target. To test this, we counted
occurrences of the string `not' (by far the most frequent negation in
the corpus). Compared to the baseline \cond{far} context, we found that participants were more likely to use negative constructions when one ($z = 27.36$) or both ($z = 34.32$) distractors were close to the target.

\newcommand{\Lex}{\mathcal{L}}
\newcommand{\Costs}{\kappa}
\newcommand{\Messages}{U}
\newcommand{\targetPrior}{P}

\begin{figure*}[t]
  \centering
  %
  \begin{subfigure}[t]{0.23\textwidth}
    \centering
    \begin{tabular}{lr@{\hskip 5pt}r@{\hskip 5pt}r@{}r}
    \toprule
     & \colorContextCompact{3884C7}{02F9FD}{9E6461}{} \\
    \midrule
    blue & \best{1}\p & \best{1}\p & 0\p \\
    teal & 0\p & \best{1}\p & 0\p \\
    dull & \best{1}\p & 0\p & \best{1}\p \\
    \bottomrule
    \end{tabular}
    \caption{The lexicon $\Lex$ defines utterances' truth-value semantics.
    Our neural listener skips the lexicon and models
    $l_0$'s probability distributions directly.}
    \label{fig:basic-rsa:lex}
  \end{subfigure}
  %
  \hfill
  %
  \begin{subfigure}[t]{0.23\textwidth}
    \centering
    \begin{tabular}{lr@{\hskip 5pt}r@{\hskip 5pt}r@{}r}
    \toprule
     & \colorContextCompact{3884C7}{02F9FD}{9E6461}{} \\
    \midrule
    blue & \best{50}\% & \best{50}\p & 0\p \\
    teal & 0\p & \best{100}\p & 0\p \\
    dull & \best{50}\p & 0\p & \best{50}\p \\
    \bottomrule
    \end{tabular}
    \caption{The literal listener $l_{0}$ chooses colors compatible with the
             literal semantics of the utterance; other than that, it
             guesses randomly.}
    \label{fig:basic-rsa:l0}
  \end{subfigure}
  %
  \hfill
  %
  \begin{subfigure}[t]{0.23\textwidth}
    \centering
    \begin{tabular}{lr@{\hskip 5pt}r@{\hskip 5pt}r@{}r}
    \toprule
     & \colorContextCompact{3884C7}{02F9FD}{9E6461}{} \\
    \midrule
    blue & \best{50}\p & 33\p & 0\p \\
    teal & 0\p & \best{67}\p & 0\p \\
    dull & \best{50}\p & 0\p & \best{100}\p \\
    \bottomrule
    \end{tabular}
    \caption{The pragmatic speaker $s_{1}$ soft-maximizes the informativity of
             its utterances. (For simplicity, $\alpha = 1$ and
             $\Costs(\utt) = 0$.)}
    \label{fig:basic-rsa:s1}
  \end{subfigure}
  %
  \hfill
  %
  \begin{subfigure}[t]{0.23\textwidth}
    \centering
    \begin{tabular}{lr@{\hskip 5pt}r@{\hskip 5pt}r@{}r}
    \toprule
     & \colorContextCompact{3884C7}{02F9FD}{9E6461}{} \\
    \midrule
    blue & \best{60}\p & 40\p & 0\p \\
    teal & 0\p & \best{100}\p & 0\p \\
    dull & 33\p & 0\p & \best{67}\p \\
    \bottomrule
    \end{tabular}
    \caption{The pragmatic listener $l_{2}$ uses Bayes' rule to infer the target
             using the speaker's utterance as evidence.}
    \label{fig:basic-rsa:l2}
  \end{subfigure}
  %
  \caption{An example of RSA applied to a color reference task (literal semantics and alternative utterances simplified for demonstration).}
  \label{fig:basic-rsa}
\end{figure*}

\paragraph{WordNet specificity}
We expect speakers to prefer basic color terms wherever they suffice
to achieve the communicative goal, since such terms are most likely to
succeed with the widest range of listeners. Thus, a speaker might
choose \word{blue} even for a clear periwinkle color. However, as the
colors get closer together, the basic terms become too ambiguous, and
thus the risk of specific terms becomes worthwhile (though lengthy
descriptions might be a safer strategy, as discussed above). To
evaluate this idea, we use WordNet \cite{Fellbaum1998} to derive a
specificity hierarchy for color terms, and we hypothesized that
\cond{split} or \cond{close} conditions will tend to lead speakers to go lower in this
hierarchy. 

For each message, we transformed adjectives into their closest noun forms (e.g. `reddish' $\rightarrow$ `red'), filtered to include only nouns with `color' in their hypernym paths, calculated the depth of the hypernym path of each color word, and took the maximum depth occurring in a message. For instance, the message ``deep magenta, purple with some pink'' received a score of 9. It has three color terms: ``purple'' and ``pink,'' which have the basic-level depth of 7, and ``magenta,'' which is a highly specific color term with a depth of 9. Finally, because there weren't meaningful differences between words at depths of 8 (``rose'', ``teal'') and 9 (``tan,'' ``taupe''), we conducted our analyses on a binary variable thresholded to distinguish ``high specificity'' messages with a depth greater than 7.
We found a small but reliable increase in the likelihood of ``high specificity'' messages from human speakers in the \cond{split}  ($z = 2.84, p = 0.005$) and \cond{close}  ($z = 2.33, p = 0.02$) contexts, compared to the baseline \cond{far} context.

\section{Models}

We first define the basic RSA model as applied to the
color reference games introduced in \secref{sec:corpus}; a worked example
is shown in \figref{fig:basic-rsa}. The starting point of RSA is a model
of a \term{literal listener}:
\begin{align}
  l_{0}(\target \| \utt, \Lex)
  &\propto
  \Lex(\utt, \target) \targetPrior(\target)
\end{align}
where $\target$ is a color in the context
set $\context$, $\utt$ is a message drawn from a set of possible
utterances $\Messages$, $\targetPrior$ is a prior over colors,
and $\Lex(\utt, \target)$ is a semantic interpretation function that
takes the value $1$ if $\utt$ is true of $\target$, else $0$.
\Figref{fig:basic-rsa:lex} shows the values of $\Lex$ defined for
a very simple context in which $\Messages=\set{\word{blue},
  \word{teal}, \word{dull}}$, and $\context =
\set{\colorContextNarrow{3884C7}{02F9FD}{9E6461}{}}$;
\figref{fig:basic-rsa:l0} shows the corresponding literal listener
$l_0$ if the prior $\targetPrior$ over colors is flat.
(In our scalable extension, we will substitute a neural network model for $l_0$,
bypassing $\Lex$ and allowing for non-binary semantic judgments.)

RSA postulates a model of a \term{pragmatic speaker} (\figref{fig:basic-rsa:s1}) that
behaves according to a distribution that soft-maximizes
a utility function rewarding informativity and penalizing cost:
\begin{align}
  s_{1}(\utt \| \target, \Lex)
  &\propto
  e^{\alpha\log(l_{0}(\target \| \utt, \Lex)) - \Costs(\utt)}
  \label{eq:rsa-s1}
\end{align}
Here, $\Costs$ is a real-valued cost function on utterances, and
$\alpha \in [0,\infty)$ is an inverse temperature parameter governing
the ``rationality'' of the speaker model.
A large $\alpha$ means the pragmatic speaker is expected to choose
the most informative utterance (minus cost)
consistently; a small $\alpha$ means the
speaker is modeled as choosing suboptimal utterances frequently.

Finally, a \term{pragmatic listener} (\figref{fig:basic-rsa:l2})
interprets utterances by reasoning about the behavior of the pragmatic speaker:
\begin{align}
  l_{2}(\target \| \utt, \Lex)
  &\propto
  s_{1}(\utt \| \target, \Lex) \targetPrior(\target)
  \label{eq:rsa-l2}
\end{align}

The $\alpha$ parameter of the speaker indirectly affects the
listener's interpretations: the more reliably the speaker
chooses the optimal utterance for a referent, the more the listener
will take deviations from the optimum as a signal to choose a
different referent.

The most important feature of this model is that the pragmatic
listener $l_{2}$ reasons not about the semantic interpretation
function $\Lex$ directly, but rather about a speaker who reasons about
a listener who reasons about $\Lex$ directly. The back-and-forth
nature of this interpretive process mirrors the
cooperative principle of conversational implicature \cite{Grice75} and
reflects more general ideas from Bayesian cognitive modeling
\cite{Tenenbaum-etal:2011}. The model and its variants have been shown
to capture a wide range of pragmatic phenomena in a cognitively
realistic manner
\cite{Goodman2013,Smith:Goodman:Frank:2013,Kao-etal:2014,Bergen:Levy:Goodman:2014},
and the central Bayesian calculation has proven useful in a variety of
communicative domains
\cite{Tellex2014a,Vogel:Potts:Jurafsky:2013}.

\begin{figure*}[t]
  \centering
  %
   \begin{subfigure}[b]{0.48\textwidth}
    \centering
    \input{input-neural-listener}
    \caption{The $\Listener_{0}$ agent processes a color description
      sequentially. The final representation is transformed into a
      Gaussian distribution in color space, which is used to score the
      context colors.}
    \label{fig:model:listener}
  \end{subfigure}
  %
  \hfill
  %
  \begin{subfigure}[b]{0.48\textwidth}
    \centering
    \input{input-neural-speaker}
    \caption{The $\Speaker_{0}$ agent processes the target color
      $c_{T}$ in context and produces a color description
      sequentially. Each step in production is conditioned by the
      final contextual representation $h$ and the previous word
      produced.}
    \label{fig:model:speaker}
  \end{subfigure}
  %
  \caption{The neural base speaker and listener agents.}
  \label{fig:model}
\end{figure*}

The definitions of $s_1$ \eqref{eq:rsa-s1} and $l_2$ \eqref{eq:rsa-l2}
give a general method of deriving a speaker from a listener and vice versa.
This suggests an alternative formulation of a pragmatic listener, starting from
a literal speaker:
\begin{align}
  s_{0}(\utt \| \target, \Lex)
  &\propto
  \Lex(\utt \| \target) e^{- \Costs(\utt)}
  \label{eq:rsa-s0} \\
  l_{1}(\target \| \utt, \Lex)
  &\propto
  s_{0}(\utt \| \target, \Lex) \targetPrior(\target)
  \label{eq:rsa-l1}
\end{align}
Here, it is the speaker that reasons about the semantics, while the listener
reasons about this speaker.

Both of these versions of RSA pose problems with
scalability, stemming from the set of messages
$\Messages$ and the interpretation function $\Lex$. In most versions of
RSA, these are specified by hand (but see \citealt{Monroe2015}).  This
presents a serious practical obstacle to applying RSA to large data
sets containing realistic utterances. The set $\Messages$
also raises a more fundamental issue: if this set is not finite (as
one would expect from a compositional grammar), then in general there is
no exact way to normalize the $s_{1}$ scores, since the denominator
must sum over all messages. The same problem applies to $s_{0}$, unless
$\Lex$ factorizes in an unrealistically clean way.

Over the next few subsections, we overcome these obstacles by
replacing $l_{0}$ and $s_0$ with RNN-based listener agents, using
the $s_0$ agent to acquire sample
utterances for approximating the normalization required in defining
the $s_{1}$ agent.

\subsection{Base listener}

Our base listener agent $\Listener_0$ (\figref{fig:model:listener}) is an LSTM encoder model that produces a Gaussian
distribution over colors in a transformed representation space (\secref{sec:color_repr}).
The input words are embedded in a 100-dimensional vector space. Word embeddings
are initialized to random normally-distributed vectors ($\mu = 0$, $\sigma = 0.01$)
and trained. The sequence of word vectors is
used as input to an LSTM with 100-dimensional hidden state, and a linear
transformation is applied to the output representation to produce a covariance matrix
and mean vector for a Gaussian distribution in color representation space.
The Gaussian scores for each of the $\contextlen$ context colors are normalized to
produce a probability distribution over the context colors. We denote this
distribution by $\Listener_0(\target \| \utt, \context; \theta)$, where $\theta$ represents the
vector of parameters that define the trained model.

\subsection{Representing colors} \label{sec:color_repr}

Each color is represented in its simplest form as a three-dimensional vector in
RGB space; we transform this to HSV (hue, saturation,
value) as a preprocessing step. These HSV vectors are then Fourier-transformed as in \newcite{MonroeGoodmanPotts16_Color} before being fed as input to the models.
The Fourier transformation maps a color
$(h, s, v)$ to a higher-dimensional\footnote{For the listener, we restrict $j,k$ to $\{0, 1\}$ and $\ell$ to 0, as we
found this improved performance slightly.} vector $\feat$:
\begin{align*}
\hat{\feat}_{jk\ell} &= \exp \left[-2\pi i \left(jh^* + ks^* + \ell v^*\right)\right] \\
\feat &= \begin{bmatrix}
  \Re{\hat{\feat}} & \Im{\hat{\feat}}
\end{bmatrix}\qquad j,k,\ell \in \{0,1,2\}
\end{align*}
where $(h^*, s^*, v^*) = (h / 360, s / 200, v/200)$.
The Fourier transformation is meant to help the models identify non-convex components
of denotations of color language, particularly periodic components.

\subsection{Base speaker}\label{sec:s0}

We also employ a LSTM-based speaker model
$\Speaker_0(\utt \| \target, \context; \phi)$. This speaker serves two purposes:
1) it is used to define a pragmatic listener akin to $l_1$ in \eqref{eq:rsa-l1},
and 2) it provides sets of alternative utterances for each context, to avoid
enumerating the intractably large space of possible utterances.

The speaker model consists of an LSTM context encoder
and an LSTM description decoder (\figref{fig:model:speaker}). In this model, the colors of the context
$\referent_i \in \context$ are transformed into Fourier representation space,
and the sequence of color representations is passed through an LSTM with
100-dimensional hidden state. The context is reordered to place the target color
last, minimizing the length of dependence between the most important input color
and the output \cite{Sutskever2014} and eliminating the need to represent the
index of the target separately.
The output of this recurrent neural network is concatenated with a
100-dimensional embedding for the previous token at each time step in decoding.
The resulting vector is input along with the previous cell state to the LSTM cell,
and an affine transformation and softmax function are applied to the output to
produce a probability distribution predicting the following token of the description.
The model is substantively similar to well-known models for image caption generation
\cite{Karpathy2015,Vinyals2015}, which use the output of a convolutional neural
network as the representation of an input image and provide this representation
to the RNN as an initial state or first word (we represent the context using
a second RNN and concatenate the context representation onto each input word vector).

\subsection{Pragmatic agents}\label{sec:l2}

Using the above base agents, we define a pragmatic speaker
$\Speaker_{1}$ and a pragmatic listener
$\Listener_{2}$:
%
\begin{align}
\Speaker_1(\utt \| \target, \context; \theta)
  &= \frac{\Listener_0(\target \| \utt, \context; \theta)^\alpha}{\sum_{\utt'}
    \Listener_0(\target \| \utt', \context; \theta)^\alpha}
    \label{eq:s1} \\
  \Listener_2(\target \| \utt, \context; \theta)
  &=
    \frac{
    \Speaker_1(\utt \| \target, \context; \theta)
    }{
    \sum_{\target'} \Speaker_1(\utt \| \target', \context; \theta)
    }
\end{align}
These definitions mirror those in \eq{eq:rsa-s1} and \eq{eq:rsa-l2}
above, with $\Lex$ replaced by the learned weights $\theta$.

Just as in \eqref{eq:rsa-s1}, the denominator in \eqref{eq:s1} should consist of a sum over
the entire set of potential utterances, which is exponentially large in the
maximum utterance length and might not even be finite.
As mentioned in \secref{sec:s0}, we limit this search by
taking $\numsamples$ samples from $\Speaker_0(\utt \| i, \context; \phi)$ for
each possible target index $i$, adding the actual utterance from the testing example,
and taking the resulting multiset as the universe of possible utterances,
weighted towards frequently-sampled utterances.\footnote{An alternative would
be to enforce uniqueness within the alternative set, keeping it a true set as in the
basic RSA formulation; this could be done with rejection sampling or beam search
for the highest-scoring speaker utterances. We found that doing so with
rejection sampling hurt model performance somewhat, so we did not
pursue the more complex beam search approach.} Taking a number
of samples from $\Speaker_0$ for each referent in the context gives the pragmatic
listener a variety of informative alternative utterances to consider when
interpreting the true input description.
In practice, we have found that $\numsamples$
can be quite small; in our experiments, it is set to $8$.

To reduce the noise
resulting from the stochastically chosen alternative utterance sets, we also perform
this alternative-set sampling $n$ times and average the resulting probabilities in
the final $\Listener_2$ output. We again choose $n = 8$ as a satisfactory
compromise between effectiveness and computation time.

A second pragmatic listener  $\Listener_1$ can be formed in a similar way from 
the neural speaker agent, analogous to $l_{1}$ in \eqref{eq:rsa-l1}:
%
\begin{align}
  \Listener_1(\target \| \utt, \context; \theta)
  &=
    \frac{
    \Speaker_0(\utt \| \target, \context; \theta)
    }{
    \sum_{\target'} \Speaker_0(\utt \| \target', \context; \theta)
    }    \label{eq:l1}
\end{align}

We expect $\Listener_1$ to be less accurate than $\Listener_0$ and
$\Listener_2$, because it is performing
a listener task using only the outputs of a model trained for a speaker task.
However, this difference in training objective can also give the model
strengths that complement those of the two listener-based agents.
One might also expect that a realistic model of human language interpretation
might lie somewhere between the ``reflex'' interpretations of
the neural base listener and the ``reasoned'' interpretations of
one of the pragmatic models. This has an intuitive justification in people's
natural uncertainty about whether their interlocutors are speaking pragmatically:
``should I read more into that statement, or take it at face value?''

We therefore also evaluate models defined as
a weighted average of $\Listener_0$ and each of $\Listener_1$ and $\Listener_2$,
as well as an ``ensemble'' model that combines all of these agents.
Specifically, we consider the following blends of neural base models and
pragmatic models:
\begin{align}
\Listener_a(\target \| \utt, \context; \theta) &\propto {\Listener_0}(\target \| \utt, \context; \theta)^{\beta_a} \cdot {} \nonumber \\
& &\mathllap{\Listener_1(\target \| \utt, \context; \theta)^{1-\beta_a}}  \label{eq:beta_a} \\
\Listener_b(\target \| \utt, \context; \theta) &\propto {\Listener_0}(\target \| \utt, \context; \theta)^{\beta_b} \cdot {} \nonumber\\
& &\mathllap{\Listener_2(\target \| \utt, \context; \theta)^{1-\beta_b}}  \label{eq:beta_b} \\
\Listener_e(\target \| \utt, \context; \theta) &\propto {\Listener_a}(\target \| \utt, \context; \theta)^{\gamma} \cdot {} \nonumber\\
& &\mathllap{\Listener_b(\target \| \utt, \context; \theta)^{1-\gamma}}  \label{eq:gamma}
\end{align}
%
The hyperparameters in the exponents allow tuning the
blend of each pair of models---e.g., overriding the neural model with the pragmatic
reasoning in $\Listener_b$. The value of the weights
$\beta_a$, $\beta_b$, and $\gamma$ can be any real number; however, we find that
good values of these weights
lie in the range [-1, 1]. As an example, setting $\beta_b = 0$ makes the
blended model $\Listener_b$ equivalent to the pragmatic model $\Listener_2$;
$\beta_b = 1$ ignores the pragmatic reasoning and uses the base model
$\Listener_0$'s outputs; and $\beta_b = -1$
``subtracts'' the base model from the pragmatic model (in log probability space)
to yield a ``hyperpragmatic'' model.

\subsection{Training} \label{sec:training}

We split our corpus into approximately equal train/dev/test sets
(15,665 train trials, 15,670 dev, 15,659 test), ensuring that trials from
the same dyad are present in only one split. 
We preprocess the data by (1) lowercasing; (2) tokenizing
by splitting off punctuation as well as the endings \word{\mbox{-er}}, \word{\mbox{-est}}, and
\word{\mbox{-ish}};\footnote{We
only apply this heuristic ending segmentation for the listener; the speaker is trained to produce
words with these endings unsegmented, to avoid segmentation inconsistencies
when passing speaker samples as alternative utterances to the listener.} and
(3) replacing tokens that appear once or not at all
in the training split\footnote{1.13\% of training tokens, 1.99\% of dev/test.} with \texttt{<unk>}. We also remove
listener utterances and concatenate speaker utterances on the same context. 
% \todocheck{[rdh: listener utterances are not already removed from the filtered corpus; we could concatenate speaker utterances too?]}
We leave handling of interactive dialogue to future work (\secref{sec:conclusion}).

We use ADADELTA
\cite{Zeiler2012} and Adam \cite{Kingma2014}, adaptive variants of
stochastic gradient descent (SGD), to train listener and speaker models.
The choice of optimization
algorithm and learning rate for each model were tuned with grid search
on a held-out tuning set consisting of 3,500 contexts.\footnote{For
  $\Listener_0$: ADADELTA, learning rate $\eta = {}$0.2; for
  $\Speaker_0$: Adam, learning rate $\alpha = {}$0.004.}
We also use a fine-grained grid search on this tuning set to determine the
values of the pragmatic reasoning parameters $\alpha$, $\beta$, and $\gamma$.
In our final ensemble $\Listener_e$, we use an inverse temperature parameter
$\alpha = 0.544$, base weights $\beta_a = 0.492$ and $\beta_b = -0.15$, and
a final blending weight $\gamma = 0.491$.
Particularly surprising is the fact that the optimal value of $\beta_b$
from grid search is \emph{negative}. The effect of this is to amplify
the difference between $\Listener_0$ and $\Listener_2$: the listener-based
pragmatic model, evidently, is not quite pragmatic enough.

\begin{comment}
\begin{table}[t]
\centering
\begin{tabular}{lrrrrrr}
  \toprule
   & \multicolumn{3}{c}{accuracy} & \multicolumn{3}{c}{perplexity} \\
   & $\Speaker_h$ & $\Speaker_0$ & $\Speaker_1$ & $\Speaker_h$ & $\Speaker_0$ & $\Speaker_1$ \\
  \midrule
  $\Listener_0$ & 83.39 & 83.66 & \oracle{99.75} & 1.69 & 1.71 & \oracle{1.03} \\
  $\Listener_b$ & \best{84.11} & \best{84.82} & \oracle{99.93} & \best{1.49} & \best{1.45} & \oracle{1.08}
  \\[0.5ex]
  $\Listener_h$ & 90.03 \\
   \bottomrule
\end{tabular}
\caption{Overall accuracy and perplexity of the base and pragmatic listeners
on inputs from various speakers, plus human accuracy. $\Speaker_h$ and $\Listener_h$
represent human agents. \oracle{Italics}: ``oracle'' results (see \secref{sec:speaker_eff}).}
\label{table:speakerVsListener}
\end{table}
\end{comment}

\section{Model results}

\subsection{Speaker behavior}

To compare human behavior with the behavior of our embedded speaker models,
we performed the same behavorial analysis done in \secref{sec:speaker_behavior}.
Results from this analysis are included alongside the human results in
\Tabref{table:metrics}.
Our pragmatic speaker model $\Speaker_1$ did not differ
qualitatively from our base speaker $\Speaker_0$ on any of the metrics,
so we only summarize results for humans and the pragmatic model.

\paragraph{Words and characters} We found human speakers to be more verbose when
colors were closer together, in both number of words and number of characters.
As \tabref{table:metrics} shows, our $\Speaker_{1}$  agent
shows the same increase in utterance length in the \cond{split} ($t = 18.07$) and \cond{close} ($t = 35.77$) contexts compared to the \cond{far} contexts.

\paragraph{Comparatives and superlatives} Humans used more comparatives and
superlatives when colors were closer together; however, comparatives were
preferred in the  \cond{split} contexts, while superlatives were preferred in
the \cond{close}
 contexts. Importantly, our pragmatic speaker shows the first of these two patterns,
producing significantly more comparatives ($z = 14.45$) and superlatives ($z = 16$) in the \cond{split} or \cond{close} conditions than in the baseline \cond{far} condition. It does not, however, capture the peak in comparative use in the \emph{split} condition. This suggests our model is simulating the human strategy at some level, but that more subtle patterns require further attention.

\paragraph{Negations} Humans used more negations when the colors were closer
together. Our pragmatic speaker's use of negation shows the  same relationship to the context ($z = 8.55$ and $z= 16.61$ respectively).

\paragraph{WordNet specificity} Humans used more ``high specificity'' words
(by WordNet hypernymy depth) when the colors were closer together.
Our pragmatic speaker showed a similar effect ($z = 2.65, p =0.008$ and
$z = 2.1, p =0.036$ respectively).

\begin{table}[t]
\centering
\begin{tabular}{lrr}
  \toprule
  model & accuracy (\%) & perplexity \\
  \midrule
  $\Listener_0$                                      & 83.30 & 1.73 \\
  $\Listener_1$ [$\Listener(\Speaker_0)$]            & 80.51 & 1.59 \\
  $\Listener_2$ [$\Listener(\Speaker(\Listener_0))$] & 83.95 & 1.51 \\
  $\Listener_a$ [$\Listener_0 \cdot \Listener_1$]    & 84.72 & 1.47 \\
  $\Listener_b$ [$\Listener_0 \cdot \Listener_2$]    & 83.98 & 1.50 \\
  $\Listener_e$ [$\Listener_a \cdot \Listener_b$]    & \best{84.84} & \best{1.45}
  \\[0.5ex]
  human & \oracle{90.40} \\
  \midrule
  $\Listener_0$                                      & 85.08 & 1.62 \\
  $\Listener_e$                                      & \best{86.98} & \best{1.39}
  \\[0.5ex]
  human & \oracle{91.08} \\
  \bottomrule
\end{tabular}
\caption{Accuracy and perplexity of the base and pragmatic listeners and
various blends (weighted averages, denoted $A \cdot B$).
Top: dev set; bottom: test set.}
\label{table:modelAccuracy}
\end{table}

\begin{figure}
\includegraphics[scale = .5]{figures/listenerAccuracy}

\includegraphics[scale = .5]{figures/changedByCondition.pdf}
\caption{Human and model reference game performance (top) and fraction of examples improved and
declined from $\Listener_0$ to $\Listener_e$ (bottom) on the dev set, by condition.}
\label{fig:listenerAccuracy}
\end{figure}

\subsection{Listener accuracy}

\Tabref{table:modelAccuracy} shows the accuracy
and perplexity of the base listener $\Listener_0$, the pragmatic listeners
$\Listener_1$ and $\Listener_2$, and the blended models $\Listener_a$,
$\Listener_b$, and $\Listener_e$ at resolving the human-written color
references. As we expected, the speaker-based $\Listener_1$ alone
performs the worst of all the models. However, blending it with $\Listener_0$
doesn't drag down $\Listener_0$'s performance but rather produces a considerable 
improvement compared to both of the original models, consistent with our
expectation that the listener-based and speaker-based models have complementary
strengths.

We observe that
$\Listener_2$ substantially outperforms its own base model $\Listener_0$,
showing that pragmatic reasoning on its own contributes positively. Blending the
pragmatic models with the base listener also improves over both individually.
Finally, the
most effective listener combines both pragmatic models with the base listener.
Plotting the number of examples changed by condition on the dev set
(\figref{fig:listenerAccuracy}) reveals that
the primary gain from including the pragmatic models is in the
\cond{close} and \cond{split} conditions, when the
model has to distinguish highly similar colors and
often cannot rely only on basic color terms.
On the test set, the final ensemble improves
significantly\footnote{$p <{}$0.001, approximate
permutation test \cite{Pado2006}, 10,000 samples} over the base model on both metrics.

\newcommand{\intended}{\framebox}

\begin{figure}[t!]
\centering
\begin{tabular}{lr@{\hskip 5pt}r@{\hskip 5pt}r@{}r}
    \toprule
%     & \multicolumn{3}{c}{$\Listener_0$} \\
    $\Listener_0$ & \colorContext{3884C7}{02F9FD}{9E6461}{} \\
    \midrule
    \textbf{blue} &             9\% & \best{   91} & $<$1 
    \\[1ex]
    true blue     & \intended{\gz{}11} & \best{     89} & $<$1 \\
    light blue    & \intended{\zz{}$<$1} & \best{$>$99} & $<$1 \\
    brightest     & $<$1 & \intended{\best{\z{}$>$99}} & $<$1 \\
    bright blue   & $<$1 & \intended{\best{\z{}$>$99}} & $<$1 \\
    red           & $<$1 &     1 & \intended{\best{\gz{}99}} \\
    purple        & $<$1 &     2 & \intended{\best{\gz{}98}} \\
    \midrule
%   & \multicolumn{3}{c}{$\Speaker_1$} \\
    $\Speaker_1$ & \colorContext{3884C7}{02F9FD}{9E6461}{}  \\
    \midrule
    \textbf{blue} &             41  &    19 & $<$1
    \\[1ex]
    true blue     & \intended{\best{\gz{}47}} &    19 & $<$1 \\
    light blue    & \intended{\gzz{}5} & \best{   20} & $<$1 \\
    brightest     & $<$1 & \intended{\best{\gz{}20}} & $<$1 \\
    bright blue   &    2 & \intended{\best{\gz{}20}} & $<$1 \\
    red           &    1 &     2 & \intended{\best{\gz{}50}} \\
    purple        &    5 &     1 & \intended{\best{\gz{}50}} \\
    \midrule
%    & \multicolumn{3}{c}{$\Listener_1$} \\
    $\Listener_2$ & \colorContext{3884C7}{02F9FD}{9E6461}{} \\
    \midrule
    \textbf{blue} & \best{            68}  &    32 & $<$1 \\
    \midrule
%    true blue     & \framebox{\best{  72}} &    29 & $<$1 \\
%    light blue    & \framebox{  21} & \best{   79} & $<$1 \\
%    brightest     & $<$1 & \framebox{\best{$>$99}} & $<$1 \\
%    bright blue   &    7 & \framebox{\best{   93}} & $<$1 \\
%    red           &    2 &     5 & \framebox{\best{  94}} \\
%    purple        &    9 &     2 & \framebox{\best{  89}} \\
%   $\Listener_b$ & \colorContext{3884C7}{02F9FD}{9E6461}{} \\
%   \midrule
%   \textbf{blue} & \best{            78}  &    22 & $<$1 \\
%   \bottomrule
\end{tabular}
\caption{Conditional probability tables used to calculate $\Listener_2$
for one dev set example. The true target color is boxed, and ``blue'' is
the human utterance. Boxed cells for alternative
utterances indicate the intended target; highest-probability referents
(listeners) and utterances (speaker) are in \textbf{bold}.
Sample sizes are reduced to save space; here,
$m = 2$ and $n = 1$ (see \secref{sec:l2}).}
\label{fig:rsaExample}
\end{figure}

Examining the full probability tables for various dev set examples on which
$\Listener_2$ is superior to $\Listener_0$ reveals a general pattern. In most
of these examples, the alternative utterances sampled from $\Speaker_0$ for one
of the referents $i$ fail to identify
their intended referent to $\Listener_0$. The pragmatic listener interprets
this to mean that referent $i$ is inherently difficult to refer to,
and it compensates by increasing referent $i$'s probability. This is beneficial
when $i$ is the true target but harmful when $i$ is a distractor.

\Figref{fig:rsaExample}
shows one such example: a context consisting of a somewhat prototypical blue,
a bright cyan, and a purple-tinged brown, with the utterance \textit{blue}. The
base listener interprets this as referring to the cyan with 91\% probability,
perhaps due to the extreme saturation of the cyan maximally activating certain
parts of the neural network. However, when the pragmatic model takes samples
from $\Speaker_0$ to probe the space of alternative utterances, it becomes
apparent that indicating the more ordinary blue to the listener is difficult:
for the utterances chosen by $\Speaker_0$ intending this referent (\textit{true blue},
\textit{light blue}), the listener also
chooses the cyan with $>$89\% confidence.

Pragmatic reasoning overcomes this difficulty. Only two utterances in the
alternative set (the actual utterance \textit{blue} and the sampled alternative
\textit{true blue}) result in any appreciable probability mass on the true target,
so the pragmatic listener's model of the speaker predicts that the speaker
would usually choose one of these two. However, if the target
were the cyan, the speaker would have many good options. Therefore, the
fact that the speaker chose \textit{blue} is interpreted as evidence for the
true target. This mirrors the back-and-forth reasoning behind the
definition of conversational implicature \cite{Grice75}.

\begin{comment}
\subsection{Speaker effectiveness} \label{sec:speaker_eff}

We also perform a ``machine communication'' experiment aimed at measuring the
ability of the speaker agents to convey the information necessary to identify
the target. \Tabref{table:speakerVsListener} shows the effectiveness of the
two speaker models---the base speaker $\Speaker_0$ (\secref{sec:s0}) and the
pragmatic speaker $\Speaker_1$ (\secref{sec:l2})---at describing the
target color to the listener models.

\ndg{can we break down according to condition as we did for human data in fig 4?}

The pragmatic speaker is nearly perfect at communicating
with the listener models, which is unsurprising, because in the case of
$\Listener_0$, the speaker has access to the listener's thought process, and in
the case of $\Listener^*$, the listener has access to the speaker's thought process.
This gives the model the opportunity to optimize its output for test set
accuracy; hence, we label results on these pairs as \textit{oracle} accuracies.

Additionally, however, the listeners are able to understand even $\Speaker_0$'s
utterances better than humans'. This is surprising, because $\Listener_0$ does not
contain an embedded model of either speaker, and $\Listener^*$ uses $\Speaker_0$
only indirectly, as a source of alternative utterances to consider.

\ndg{this will leave people confused.}

\subsection{Tuning the pragmatic parameters} \label{sec:alpha_beta}

As noted in \secref{sec:training}, we use an inverse temperature parameter of
$\alpha = 0.544$ and a base weight of $\beta = -0.15$ for the pragmatic
listener. Particularly surprising is the fact that the optimal value of $\beta$
discovered in grid search is \emph{negative}. This has the effect of amplifying
the difference between $\Listener_0$ and $\Listener_1$: the pragmatic model,
evidently, is not quite pragmatic enough.

The $\alpha$ value of 0.544${}<{}$1 indicates that the pragmatic listener is
modeling a somewhat less reliable speaker than one that uses a straightforward
softmax choice distribution. Empirical values of $\alpha$ fitting human behavior
in prior work with RSA-based models have varied from task to task;
\newcite{Kao-etal:2014} give $\alpha = 0.36$ for predicting
hyperbole in numeric expressions, while \newcite{Graf2016} find $\alpha = 10.8$ for
predicting hypernymy level in object references.

\begin{figure}
\centering
\includegraphics[width=\columnwidth]{figures/alpha_beta.eps}
\caption{Perplexity $\pi$ on tuning set as a function of $\alpha$ and $\beta$, plotted as $\log(\pi - \min_{\alpha,\beta} \pi(\alpha, \beta))$.}
\label{fig:alpha_beta}
\end{figure}

Grid search over values of $\alpha$ and $\beta$ revealed that the optimal values of
the two parameters are related (\figref{fig:alpha_beta}). Good combinations lay on
a curve with $\alpha$ decreasing as $\beta$ increased; however, negative $\beta$ and
very small $\alpha$ was catastrophic to perplexity.\footnote{We tuned the
parameters on perplexity rather than accuracy because accuracy was noisy and had
many local and global minima.}

\ndg{this section could be cut or shortened...}
\end{comment}

\section{Related work}

Prior work combining machine learning with probabilistic pragmatic reasoning
models has largely focused on the speaker side, i.e., generation.
\newcite{Golland2010} develop a pragmatic speaker model,
$\Speaker(\Listener_0)$, that reasons about log-linear listeners trained on human
utterances containing spatial references in virtual-world environments.
\newcite{Tellex2014a} apply a similar technique, under the name
\term{inverse semantics}, to create a robot that can informatively ask
humans for assistance in accomplishing tasks. \newcite{Monroe2015} implement
an end-to-end trained $\Speaker(\Listener(\Speaker_0))$ model for referring
expression generation in a reference game task; their model requires enumerating
the set of possible utterances for each context, which is infeasible when
utterances are as varied as those in our dataset.

The closest work to ours that we are aware of is that of
\newcite{AndreasKlein16_NeuralPragmatics}, who also combine neural speaker
and listener models in a reference game setting. They propose a
pragmatic speaker, $\Speaker(\Listener_0)$, sampling from a neural
$\Speaker_0$ model to limit the search space and regularize the model toward
human-like utterances. We show these techniques help in
listener (understanding) tasks as well. Approaching pragmatics from the listener
side requires either inverting the pragmatic reasoning (i.e., deriving a
listener from a speaker), or adding another step of recursive reasoning,
yielding a two-level derived pragmatic model
$\Listener(\Speaker(\Listener_0))$. We show both approaches contribute
to an effective listener.

\begin{comment}
Additionally, we add the $\alpha$ inverse temperature parameter from
\newcite{Goodman2013}, and we replace the multi-layer perceptron models over
$n$-gram features used in \newcite{AndreasKlein16_NeuralPragmatics}
with recurrent models for both the base listener and the base speaker.
These sequential models can approximate the compositional semantics
that is so crucial for operators like negation and comparatives.
Our speaker is also more expressive in that it is able to capture
properties of the context when sampling alternative utterances.
\end{comment}

\section{Conclusion} \label{sec:conclusion}

In this paper, we present a newly-collected corpus of color descriptions from
reference games, and we show that a pragmatic reasoning agent
incorporating neural listener and speaker models
interprets color descriptions in context better than the listener alone.

Our pragmatic reasoning scheme is applied on
top of pre-trained models. Since the human data is presumed to be pragmatic,
we expect that training a model in an end-to-end fashion as in
\newcite{Monroe2015} would be advantageous.
Doing so would require a more principled solution to the
problem of the intractable alternative utterance space.

Another area of investigation we did not pursue in this work is
multi-turn dialogue. As noted in \secref{sec:corpus},
both participants in our reference game task could use the chat window
at any point, and more than half of dyads
had at least one two-way interaction. Dialogue agents are more
challenging to model than isolated speakers and listeners,
requiring long-term planning, remembering previous utterances, and
(for the listener) deciding when to keep talking or commit to a referent
\cite{Lewis79_Scorekeeping,BrownYule83_Discourse,Clark96,Roberts96_InformationStructureDiscourse}.
We release our
dataset\footnote{\label{foot:release}\url{http://see-supplementary-material/}}
with the expectation that others may find interest in these challenges as well.


%\section{Credits}
%
%This document has been adapted from the instructions for ACL proceedings, including those for ACL-2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui, those for ACL-2005 by Hwee Tou Ng and Kemal Oflazer, those for ACL-2002 by Eugene Charniak and Dekang Lin, and earlier ACL and EACL formats. Those versions were written by several people, including John Chen, Henry S. Thompson and Donald Walker. Additional elements were taken from the formatting instructions of the {\em International Joint Conference on Artificial Intelligence}.
%
%\section{Introduction}
%
%The following instructions are directed to authors of papers submitted to TACL or accepted for publication. All authors are required to adhere to these specifications. Authors are required to provide a Portable Document Format (PDF)
%% das: removed reference to PostScript
%% and PostScript
%version of their papers. \textbf{The proceedings will be printed on
%US-Letter paper}. Authors from countries in which access to
%word-processing systems is limited should contact TACL editors at editors-in-chief@transacl.org as soon as possible.
%
%
%\section{General Instructions}
%
%Manuscripts must be in two-column format. Exceptions to the two-column format include the title,
%authors' names and complete addresses, which must be centered at the top of the first page,
%and any full-width figures or tables (see the guidelines in Subsection~\ref{ssec:first}). {\bf Type single-spaced}.
%Start all pages directly under the top margin. See the guide-lines later regarding formatting the first page. Do not number the pages.
%
%\subsection{Electronically-available resources}
%
%TACL provides this description in \LaTeX2e (tacl.tex) and PDF format (tacl.pdf), along with the LATEX2e style file used to format it (acl2012.sty) and an ACL bibliography style (acl2012.bst).  A Microsoft Word template file (tacl.dot) is also available. We require the use of these style files, which have been appropriately tailored for TACL. If you have an option, we recommend that you use the \LaTeX2e version. \textbf{If you will be using the Microsoft Word template, we suggest that you anonymize your source file so that the pdf produced does not retain your identity.} This can be done by removing any personal information from your source
%document properties.
%
%
%\subsection{Format of Electronic Manuscript}
%\label{sect:pdf}
%
%For the production of the electronic manuscript you must use Adobe's
%Portable Document Format (PDF). This format can be generated from
%postscript files: on Linux/Unix systems, you can use {\tt ps2pdf} for this
%purpose; under Microsoft Windows, you can use Adobe's Distiller, or
%if you have {\tt cygwin} installed, you can use {\tt dvipdf} or
%{\tt ps2pdf}.  Note
%that some word processing programs generate PDF which may not include
%all the necessary fonts (esp. tree diagrams, symbols). When you print
%or create the PDF file, there is usually an option in your printer
%setup to include none, all or just non-standard fonts.  Please make
%sure that you select the option of including ALL the fonts.  {\em Before sending it, test your PDF by printing it from a computer different from the one where it was created}. Moreover,
%some word processor may generate very large postscript/PDF files,
%where each page is rendered as an image. Such images may reproduce
%poorly.  In this case, try alternative ways to obtain the postscript
%and/or PDF.  One way on some systems is to install a driver for a
%postscript printer, send your document to the printer specifying
%``Output to a file'', then convert the file to PDF.
%
%Additionally, it is of utmost importance to specify the {\bf US-Letter format} (8.5in $\times$ 11in) when formatting the paper. When working with {\tt dvips}, for instance, one should specify {\tt -t letter}.
%
%Print-outs of the PDF file on US-Letter paper should be identical to the
%hardcopy version.  If you cannot meet the above requirements about the
%production of your electronic submission, please contact the
%publication chair above as soon as possible.
%
%
%\subsection{Layout}
%\label{ssec:layout}
%
%Format manuscripts two columns to a page, in the manner these
%instructions are formatted. The exact dimensions for a page on US-letter
%paper are:
%
%\begin{itemize}
%\item Left and right margins: 1in
%\item Top margin:1in
%\item Bottom margin: 1in
%\item Column width: 3.15in
%\item Column height: 9in
%\item Gap between columns: 0.2in
%\end{itemize}
%
%\noindent Papers should not be submitted on any other paper size. If you cannot meet the above requirements about the production of your electronic submission, please contact the publication chair above as soon as possible.
%
%\subsection{Fonts}
%
%For reasons of uniformity, Adobe's {\bf Times Roman} font should be
%used. In \LaTeX2e{} this is accomplished by putting
%
%\begin{quote}
%\begin{verbatim}
%\usepackage{times}
%\usepackage{latexsym}
%\end{verbatim}
%\end{quote}
%in the preamble. If Times Roman is unavailable, use {\bf Computer
%Modern Roman} (\LaTeX2e{}'s default).  Note that the latter is about
%10\% less dense than Adobe's Times Roman font.
%
%
%\begin{table}[h]
%\begin{center}
%\begin{tabular}{|l|rl|}
%\hline \bf Type of Text & \bf Font Size & \bf Style \\ \hline
%paper title & 15 pt & bold \\
%author names & 12 pt & bold \\
%author affiliation & 12 pt & \\
%the word ``Abstract'' & 12 pt & bold \\
%section titles & 12 pt & bold \\
%document text & 11 pt  &\\
%captions & 10 pt & \\
%abstract text & 10 pt & \\
%bibliography & 10 pt & \\
%footnotes & 9 pt & \\
%\hline
%\end{tabular}
%\end{center}
%\caption{\label{font-table} Font guide. }
%\end{table}
%
%\subsection{The First Page}
%\label{ssec:first}
%
%Center the title, author's name(s) and affiliation(s) across both
%columns. Do not use footnotes for affiliations.  Do not include the
%paper ID number assigned during the submission process.
%Use the two-column format only when you begin the abstract.
%
%{\bf Title}: Place the title centered at the top of the first page, in
%a 15 point bold font.  (For a complete guide to font sizes and styles, see Table~\ref{font-table}.)
%Long title should be typed on two lines without
%a blank line intervening. Approximately, put the title at 1in from the
%top of the page, followed by a blank line, then the author's names(s),
%and the affiliation on the following line.  Do not use only initials
%for given names (middle initials are allowed). Do not format surnames
%in all capitals (e.g., ``Zhou,'' not ``ZHOU'').  The affiliation should
%contain the author's complete address, and if possible an electronic
%mail address. Leave about 0.75in between the affiliation and the body
%of the first page. The title, author names and addresses should be completely identical to those entered to the electronic paper submission website in order to maintain the consistency of author information among all publications of the conference.
%
%{\bf Abstract}: Type the abstract at the beginning of the first
%column.  The width of the abstract text should be smaller than the
%width of the columns for the text in the body of the paper by about
%0.25in on each side.  Center the word {\bf Abstract} in a 12 point
%bold font above the body of the abstract. The abstract should be a
%concise summary of the general thesis and conclusions of the paper.
%It should be no longer than 200 words. The abstract text should be in 10 point font.
%
%{\bf Text}: Begin typing the main body of the text immediately after
%the abstract, observing the two-column format as shown in
%the present document. Do not include page numbers.
%
%{\bf Indent} when starting a new paragraph. For reasons of uniformity,
%use Adobe's {\bf Times Roman} fonts, with 11 points for text and
%subsection headings, 12 points for section headings and 15 points for
%the title.  If Times Roman is unavailable, use {\bf Computer Modern
%Roman} (\LaTeX2e's default; see section \ref{sect:pdf} above).
%Note that the latter is about 10\% less dense than Adobe's Times Roman
%font.
%
%\subsection{Sections}
%
%{\bf Headings}: Type and label section and subsection headings in the
%style shown on the present document.  Use numbered sections (Arabic
%numerals) in order to facilitate cross references. Number subsections
%with the section number and the subsection number separated by a dot,
%in Arabic numerals. Do not number subsubsections.
%
%{\bf Citations}: Citations within the text appear
%in parentheses as~\cite{Gusfield:97} or, if the author's name appears in
%the text itself, as Gusfield~\shortcite{Gusfield:97}. Append lowercase letters to the year in cases of ambiguities. Treat double authors as in~\cite{Aho:72}, but write as in~\cite{Chandra:81} when more than two authors are involved. Collapse multiple citations as in~\cite{Gusfield:97,Aho:72}. Also refrain from using full citations as sentence constituents. We suggest that instead of
%\begin{quote}
%``\cite{Gusfield:97} showed that ...''
%\end{quote}
%you use
%\begin{quote}
%``Gusfield \shortcite{Gusfield:97}   showed that ...''
%\end{quote}
%
%If you are using the provided \LaTeX{} and Bib\TeX{} style files, you
%can use the command \verb|\newcite| to get ``author (year)'' citations.
%
%As reviewing will be double-blind (except that action editors know author identity and authors know action-editor identity), the submitted version of the papers should not include the
%authors' names and affiliations. Furthermore, self-references that
%reveal the author's identity, e.g.,
%\begin{quote}
%``We previously showed \cite{Gusfield:97} ...''
%\end{quote}
%should be avoided. Instead, use citations such as
%\begin{quote}
%``Gusfield \shortcite{Gusfield:97}
%previously showed ... ''
%\end{quote}
%
%To be clear: You should reference your prior work if it is relevant; but use the third person instead of the 1st person and in place of references like “(Anonymous) showed…”, since such anonymized references do not allow readers to examine relevant related work.
%
%Authors’ names should also be removed from the ``Document Properties'' display that can be viewed using Adobe Acrobat’s ``File $\rightarrow$ Properties'' menu.
%
%Please do not include acknowledgements when submitting your papers. Papers that do not conform
%to these requirements may be rejected without review.
%
%\textbf{References}: Gather the full set of references together under
%the heading {\bf References}; place the section before any Appendices,
%unless they contain references. Arrange the references alphabetically
%by first author, rather than by order of occurrence in the text.
%Provide as complete a citation as possible, using a consistent format,
%such as the one for {\em Computational Linguistics\/} or the one in the
%{\em Publication Manual of the American
%Psychological Association\/}~\cite{APA:83}.  Use of full names for
%authors rather than initials is preferred.  A list of abbreviations
%for common computer science journals can be found in the ACM
%{\em Computing Reviews\/}~\cite{ACM:83}.
%
%The \LaTeX{} and Bib\TeX{} style files provided roughly fit the
%American Psychological Association format, allowing regular citations,
%short citations and multiple citations as described above.
%
%{\bf Appendices}: Appendices, if any, directly follow the text and the
%references (but see above).  Letter them in sequence and provide an
%informative title: {\bf Appendix A. Title of Appendix}.
%
%\textbf{Acknowledgment} sections should go as a last section immediately
%before the references. Do not number the acknowledgement section.
%
%\subsection{Footnotes}
%
%{\bf Footnotes}: Put footnotes at the bottom of the page. They may
%be numbered or referred to by asterisks or other
%symbols.\footnote{This is how a footnote should appear.} Footnotes
%should be separated from the text by a line.\footnote{Note the
%line separating the footnotes from the text.}  Footnotes should be in 9 point font.
%
%\subsection{Graphics}
%
%{\bf Illustrations}: Place figures, tables, and photographs in the
%paper near where they are first discussed, rather than at the end, if
%possible.  Wide illustrations may run across both columns and should be placed at
%the top of a page. Color illustrations are discouraged, unless you have verified that
%they will be understandable when printed in black ink.
%
%{\bf Captions}: Provide a caption for every illustration; number each one
%sequentially in the form:  ``Figure 1. Caption of the Figure.'' ``Table 1.
%Caption of the Table.''  Type the captions of the figures and
%tables below the body, using 10 point text.
%
%\section{Translation of non-English Terms}
%
%It is also advised to supplement non-English characters and terms
%with appropriate transliterations and/or translations
%since not all readers understand all such characters and terms.
%
%Inline transliteration or translation can be represented in
%the order of: original-form transliteration ``translation''.
%
%\section{Length of Submission}
%\label{sec:length}
%
%Submissions may consist of seven to ten (7-10) letter format (not A4) pages of content and unlimited additional pages (only) allowed for references. Papers that are revisions of submissions with prior (b) or (c) decisions may be allowed one to two additional pages of content to accommodate required revisions.
%
%Appendices (if any) are counted as content pages. Papers that do not conform to the specified length and
%formatting requirements are subject to re-submission.
%
%
%\section*{Acknowledgments}
%
%Do not number the acknowledgment section. Do not include this section when submitting your paper for review.

\bibliography{colors}
\bibliographystyle{acl2012}

\end{document}
